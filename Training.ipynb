{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/media/arg_ws3/5E703E3A703E18EB/data/subt_real\"\n",
    "DATASET_NAME = \"subt_artifact\"\n",
    "cfg = subt\n",
    "BASE_NET = \"./weights/vgg16_reducedfc.pth\"\n",
    "DATA_DETECTION = SUBTDetection\n",
    "BATCH_SIZE = 32\n",
    "#PRETRAINED_MODEL = \"/home/arg_ws3/ssd.pytorch/weights/person/person_67500.pth\"\n",
    "PRETRAINED_MODEL = None\n",
    "PRETRAINED_EPOCH = 0\n",
    "SAVE_MODEL_EPOCH = 5\n",
    "START_ITER = 0\n",
    "NUM_WORKERS = 8\n",
    "EPOCH = 61\n",
    "adjust_lr_epoch = [15, 30, 45]\n",
    "CUDA = True\n",
    "LR = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "GAMMA = 0.1\n",
    "VISDOM = False\n",
    "SAVE_FOLDER = \"/media/arg_ws3/5E703E3A703E18EB/data/subt_real/model/\"\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if not CUDA:\n",
    "        print(\"WTF are u wasting your CUDA device?\")\n",
    "    else:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "# Initial model weights & bias\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "# Adjust learning rate during training\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step \n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    #lr = LR * (gamma ** (step))\n",
    "    lr = LR * (gamma)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        print(\"Change learning rate to: \", lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index: \n",
      " {'bb_extinguisher': 0, 'bb_drill': 1}\n",
      "['bb_extinguisher', 'bb_drill', 'None']\n"
     ]
    }
   ],
   "source": [
    "dataset = DATA_DETECTION(root=DATASET_ROOT, image_sets=['train'],transform=SSDAugmentation(cfg['min_dim'], MEANS))\n",
    "\n",
    "classes = dataset.target_transform.class_to_ind\n",
    "print(\"Class to index: \\n\", classes)\n",
    "classes = sorted(classes.items(), key=lambda kv: kv[1])\n",
    "label = []\n",
    "for i in classes:\n",
    "    label.append(i[0])\n",
    "label.append('None')\n",
    "print(label)\n",
    "true_label = ['person', 'palm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
      "/home/arg_ws3/ssd.pytorch/layers/modules/l2norm.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.weight,self.gamma)\n"
     ]
    }
   ],
   "source": [
    "ssd_pretrained = build_ssd('train', cfg['min_dim'], 3)\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_pretrained)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "    ssd_pretrained.load_weights(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Delcare SSD Network\n",
    "ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "net = ssd_net\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "SAME_CLASS = False\n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    if SAME_CLASS:\n",
    "        print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "        ssd_net.load_weights(PRETRAINED_MODEL)\n",
    "    else:\n",
    "        print('Load pretrained model with different classes')\n",
    "        ssd_pretrained = build_ssd('train', cfg['min_dim'], 2)\n",
    "        ssd_pretrained.load_weights(PRETRAINED_MODEL)\n",
    "        ssd_net.vgg = ssd_pretrained.vgg\n",
    "        ssd_net.extras = ssd_pretrained.extras\n",
    "        ssd_net.loc = ssd_pretrained.loc\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "else:\n",
    "    print('Initializing weights...')\n",
    "    vgg_weights = torch.load(BASE_NET) # load vgg pretrained model\n",
    "    ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "    ssd_net.extras.apply(weights_init) # Initial SSD model weights & bias\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM,\n",
    "                weight_decay=WEIGHT_DECAY)\n",
    "criterion = MultiBoxLoss(BATCH_SIZE ,cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                False, CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(list(ssd_net.parameters()))-30):\n",
    "#    list(ssd_net.parameters())[i].data = list(ssd_pretrained.parameters())[i]\n",
    "#list(ssd_net.conf)[-1] = list(ssd_pretrained.conf)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Training SSD on: subt_artifact\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "# loss counters\n",
    "loc_loss = 0\n",
    "conf_loss = 0\n",
    "epoch = 0\n",
    "print('Loading the dataset...')\n",
    "epoch_size = len(dataset) // BATCH_SIZE\n",
    "print('Training SSD on:', DATASET_NAME)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, BATCH_SIZE,\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                shuffle=True, collate_fn=detection_collate,\n",
    "                                pin_memory=True)\n",
    "batch_iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 0.3040 sec.\n",
      "Epoch: 0 || iter 8 || Loss: 14.1264 ||timer: 0.3065 sec.\n",
      "Epoch: 0 || iter 16 || Loss: 8.9240 ||timer: 0.3063 sec.\n",
      "Epoch: 0 || iter 24 || Loss: 8.0102 ||timer: 0.3316 sec.\n",
      "Epoch: 0 || iter 32 || Loss: 7.7912 ||timer: 0.3311 sec.\n",
      "Epoch: 0 || iter 40 || Loss: 7.1676 ||timer: 0.3321 sec.\n",
      "Epoch: 0 || iter 48 || Loss: 6.9750 ||timer: 0.3316 sec.\n",
      "Epoch: 0 || iter 56 || Loss: 6.9701 ||timer: 0.3307 sec.\n",
      "Epoch: 0 || iter 64 || Loss: 6.7572 ||timer: 0.3307 sec.\n",
      "Epoch: 0 || iter 72 || Loss: 6.2997 ||timer: 0.3062 sec.\n",
      "Epoch: 1 || iter 80 || Loss: 6.3721 ||timer: 0.3064 sec.\n",
      "Epoch: 1 || iter 88 || Loss: 6.2638 ||timer: 0.3351 sec.\n",
      "Epoch: 1 || iter 96 || Loss: 6.4558 ||timer: 0.3324 sec.\n",
      "Epoch: 1 || iter 104 || Loss: 5.9591 ||timer: 0.3315 sec.\n",
      "Epoch: 1 || iter 112 || Loss: 6.1522 ||timer: 0.3324 sec.\n",
      "Epoch: 1 || iter 120 || Loss: 6.2604 ||timer: 0.3301 sec.\n",
      "Epoch: 1 || iter 128 || Loss: 6.2278 ||timer: 0.3295 sec.\n",
      "Epoch: 1 || iter 136 || Loss: 5.8336 ||timer: 0.3308 sec.\n",
      "Epoch: 1 || iter 144 || Loss: 5.7058 ||timer: 0.3352 sec.\n",
      "Epoch: 2 || iter 152 || Loss: 5.6104 ||timer: 0.3334 sec.\n",
      "Epoch: 2 || iter 160 || Loss: 5.7497 ||timer: 0.3319 sec.\n",
      "Epoch: 2 || iter 168 || Loss: 5.7527 ||timer: 0.3305 sec.\n",
      "Epoch: 2 || iter 176 || Loss: 5.8396 ||timer: 0.3319 sec.\n",
      "Epoch: 2 || iter 184 || Loss: 5.7735 ||timer: 0.3324 sec.\n",
      "Epoch: 2 || iter 192 || Loss: 5.4378 ||timer: 0.3339 sec.\n",
      "Epoch: 2 || iter 200 || Loss: 5.3217 ||timer: 0.3309 sec.\n",
      "Epoch: 2 || iter 208 || Loss: 5.5059 ||timer: 0.3308 sec.\n",
      "Epoch: 2 || iter 216 || Loss: 5.3380 ||timer: 0.3264 sec.\n",
      "Epoch: 3 || iter 224 || Loss: 5.0857 ||timer: 0.3326 sec.\n",
      "Epoch: 3 || iter 232 || Loss: 5.2425 ||timer: 0.3306 sec.\n",
      "Epoch: 3 || iter 240 || Loss: 5.1257 ||timer: 0.3308 sec.\n",
      "Epoch: 3 || iter 248 || Loss: 4.9924 ||timer: 0.3334 sec.\n",
      "Epoch: 3 || iter 256 || Loss: 4.5173 ||timer: 0.3314 sec.\n",
      "Epoch: 3 || iter 264 || Loss: 4.6967 ||timer: 0.3322 sec.\n",
      "Epoch: 3 || iter 272 || Loss: 4.6152 ||timer: 0.3302 sec.\n",
      "Epoch: 3 || iter 280 || Loss: 4.9262 ||timer: 0.3290 sec.\n",
      "Epoch: 3 || iter 288 || Loss: 4.0368 ||timer: 0.3239 sec.\n",
      "Epoch: 4 || iter 296 || Loss: 4.4426 ||timer: 0.3327 sec.\n",
      "Epoch: 4 || iter 304 || Loss: 3.7762 ||timer: 0.3320 sec.\n",
      "Epoch: 4 || iter 312 || Loss: 4.1519 ||timer: 0.3300 sec.\n",
      "Epoch: 4 || iter 320 || Loss: 3.6563 ||timer: 0.3312 sec.\n",
      "Epoch: 4 || iter 328 || Loss: 3.7504 ||timer: 0.3324 sec.\n",
      "Epoch: 4 || iter 336 || Loss: 3.8374 ||timer: 0.3323 sec.\n",
      "Epoch: 4 || iter 344 || Loss: 3.4728 ||timer: 0.3308 sec.\n",
      "Epoch: 4 || iter 352 || Loss: 3.8327 ||timer: 0.3308 sec.\n",
      "Epoch: 4 || iter 360 || Loss: 3.5670 ||timer: 0.3169 sec.\n",
      "Epoch: 5 || iter 368 || Loss: 3.2835 ||timer: 0.3311 sec.\n",
      "Epoch: 5 || iter 376 || Loss: 3.3485 ||timer: 0.3314 sec.\n",
      "Epoch: 5 || iter 384 || Loss: 3.3593 ||timer: 0.3298 sec.\n",
      "Epoch: 5 || iter 392 || Loss: 3.1863 ||timer: 0.3324 sec.\n",
      "Epoch: 5 || iter 400 || Loss: 3.2230 ||timer: 0.3327 sec.\n",
      "Epoch: 5 || iter 408 || Loss: 3.2209 ||timer: 0.3331 sec.\n",
      "Epoch: 5 || iter 416 || Loss: 3.4749 ||timer: 0.3297 sec.\n",
      "Epoch: 5 || iter 424 || Loss: 3.5030 ||timer: 0.3733 sec.\n",
      "Epoch: 5 || iter 432 || Loss: 3.0665 ||timer: 0.3940 sec.\n",
      "Epoch: 6 || iter 440 || Loss: 3.3871 ||timer: 0.3309 sec.\n",
      "Epoch: 6 || iter 448 || Loss: 2.9387 ||timer: 0.3318 sec.\n",
      "Epoch: 6 || iter 456 || Loss: 3.1142 ||timer: 0.3348 sec.\n",
      "Epoch: 6 || iter 464 || Loss: 3.0185 ||timer: 0.3320 sec.\n",
      "Epoch: 6 || iter 472 || Loss: 3.2597 ||timer: 0.3342 sec.\n",
      "Epoch: 6 || iter 480 || Loss: 2.9787 ||timer: 0.3299 sec.\n",
      "Epoch: 6 || iter 488 || Loss: 3.3398 ||timer: 0.3297 sec.\n",
      "Epoch: 6 || iter 496 || Loss: 3.1087 ||timer: 0.3304 sec.\n",
      "Epoch: 6 || iter 504 || Loss: 3.0847 ||timer: 0.3459 sec.\n",
      "Epoch: 7 || iter 512 || Loss: 2.6099 ||timer: 0.3302 sec.\n",
      "Epoch: 7 || iter 520 || Loss: 2.8200 ||timer: 0.3322 sec.\n",
      "Epoch: 7 || iter 528 || Loss: 3.2125 ||timer: 0.3332 sec.\n",
      "Epoch: 7 || iter 536 || Loss: 2.8329 ||timer: 0.3322 sec.\n",
      "Epoch: 7 || iter 544 || Loss: 2.4934 ||timer: 0.3324 sec.\n",
      "Epoch: 7 || iter 552 || Loss: 3.3035 ||timer: 0.3329 sec.\n",
      "Epoch: 7 || iter 560 || Loss: 2.7377 ||timer: 0.3294 sec.\n",
      "Epoch: 7 || iter 568 || Loss: 2.7502 ||timer: 0.3294 sec.\n",
      "Epoch: 7 || iter 576 || Loss: 3.5120 ||timer: 0.1207 sec.\n",
      "Epoch: 7 || iter 584 || Loss: 2.9084 ||timer: 0.3319 sec.\n",
      "Epoch: 8 || iter 592 || Loss: 2.6523 ||timer: 0.3326 sec.\n",
      "Epoch: 8 || iter 600 || Loss: 2.6883 ||timer: 0.3314 sec.\n",
      "Epoch: 8 || iter 608 || Loss: 2.7198 ||timer: 0.3321 sec.\n",
      "Epoch: 8 || iter 616 || Loss: 3.1451 ||timer: 0.3321 sec.\n",
      "Epoch: 8 || iter 624 || Loss: 2.6154 ||timer: 0.3349 sec.\n",
      "Epoch: 8 || iter 632 || Loss: 2.7373 ||timer: 0.3310 sec.\n",
      "Epoch: 8 || iter 640 || Loss: 2.5619 ||timer: 0.3292 sec.\n",
      "Epoch: 8 || iter 648 || Loss: 2.6831 ||timer: 0.3294 sec.\n",
      "Epoch: 8 || iter 656 || Loss: 2.4068 ||timer: 0.3315 sec.\n",
      "Epoch: 9 || iter 664 || Loss: 2.8952 ||timer: 0.3336 sec.\n",
      "Epoch: 9 || iter 672 || Loss: 2.7466 ||timer: 0.3321 sec.\n",
      "Epoch: 9 || iter 680 || Loss: 2.7572 ||timer: 0.3326 sec.\n",
      "Epoch: 9 || iter 688 || Loss: 2.5014 ||timer: 0.3312 sec.\n",
      "Epoch: 9 || iter 696 || Loss: 2.6970 ||timer: 0.3303 sec.\n",
      "Epoch: 9 || iter 704 || Loss: 2.6568 ||timer: 0.3320 sec.\n",
      "Epoch: 9 || iter 712 || Loss: 2.8703 ||timer: 0.3308 sec.\n",
      "Epoch: 9 || iter 720 || Loss: 2.7972 ||timer: 0.3305 sec.\n",
      "Epoch: 9 || iter 728 || Loss: 2.7500 ||timer: 0.3289 sec.\n",
      "Epoch: 10 || iter 736 || Loss: 2.6388 ||timer: 0.3324 sec.\n",
      "Epoch: 10 || iter 744 || Loss: 2.3981 ||timer: 0.3358 sec.\n",
      "Epoch: 10 || iter 752 || Loss: 2.5142 ||timer: 0.3321 sec.\n",
      "Epoch: 10 || iter 760 || Loss: 2.7362 ||timer: 0.3307 sec.\n",
      "Epoch: 10 || iter 768 || Loss: 2.5286 ||timer: 0.3347 sec.\n",
      "Epoch: 10 || iter 776 || Loss: 2.7495 ||timer: 0.3663 sec.\n",
      "Epoch: 10 || iter 784 || Loss: 2.4612 ||timer: 0.3373 sec.\n",
      "Epoch: 10 || iter 792 || Loss: 2.3970 ||timer: 0.3292 sec.\n",
      "Epoch: 10 || iter 800 || Loss: 2.5987 ||Saving state, Epoch: 10\n",
      "timer: 0.3269 sec.\n",
      "Epoch: 11 || iter 808 || Loss: 2.4559 ||timer: 0.3325 sec.\n",
      "Epoch: 11 || iter 816 || Loss: 2.5583 ||timer: 0.3321 sec.\n",
      "Epoch: 11 || iter 824 || Loss: 2.3788 ||timer: 0.3306 sec.\n",
      "Epoch: 11 || iter 832 || Loss: 2.6028 ||timer: 0.3314 sec.\n",
      "Epoch: 11 || iter 840 || Loss: 1.8841 ||timer: 0.3317 sec.\n",
      "Epoch: 11 || iter 848 || Loss: 2.5145 ||timer: 0.3336 sec.\n",
      "Epoch: 11 || iter 856 || Loss: 2.6936 ||timer: 0.3319 sec.\n",
      "Epoch: 11 || iter 864 || Loss: 2.4153 ||timer: 0.3300 sec.\n",
      "Epoch: 11 || iter 872 || Loss: 2.7143 ||timer: 0.3250 sec.\n",
      "Epoch: 12 || iter 880 || Loss: 2.4974 ||timer: 0.3303 sec.\n",
      "Epoch: 12 || iter 888 || Loss: 2.7879 ||timer: 0.3786 sec.\n",
      "Epoch: 12 || iter 896 || Loss: 2.1666 ||timer: 0.3807 sec.\n",
      "Epoch: 12 || iter 904 || Loss: 2.3463 ||timer: 0.3328 sec.\n",
      "Epoch: 12 || iter 912 || Loss: 2.2283 ||timer: 0.3713 sec.\n",
      "Epoch: 12 || iter 920 || Loss: 2.0546 ||timer: 0.3594 sec.\n",
      "Epoch: 12 || iter 928 || Loss: 2.6291 ||timer: 0.3740 sec.\n",
      "Epoch: 12 || iter 936 || Loss: 1.8866 ||timer: 0.3583 sec.\n",
      "Epoch: 12 || iter 944 || Loss: 2.2144 ||timer: 0.3150 sec.\n",
      "Epoch: 13 || iter 952 || Loss: 2.3954 ||timer: 0.3338 sec.\n",
      "Epoch: 13 || iter 960 || Loss: 2.5054 ||timer: 0.3298 sec.\n",
      "Epoch: 13 || iter 968 || Loss: 2.6104 ||timer: 0.3323 sec.\n",
      "Epoch: 13 || iter 976 || Loss: 2.2385 ||timer: 0.3328 sec.\n",
      "Epoch: 13 || iter 984 || Loss: 3.0066 ||timer: 0.3335 sec.\n",
      "Epoch: 13 || iter 992 || Loss: 2.5030 ||timer: 0.3307 sec.\n",
      "Epoch: 13 || iter 1000 || Loss: 2.1243 ||timer: 0.3309 sec.\n",
      "Epoch: 13 || iter 1008 || Loss: 2.7383 ||timer: 0.3295 sec.\n",
      "Epoch: 13 || iter 1016 || Loss: 2.3674 ||timer: 0.3576 sec.\n",
      "Epoch: 14 || iter 1024 || Loss: 2.5370 ||timer: 0.3326 sec.\n",
      "Epoch: 14 || iter 1032 || Loss: 2.1954 ||timer: 0.3316 sec.\n",
      "Epoch: 14 || iter 1040 || Loss: 2.6584 ||timer: 0.3306 sec.\n",
      "Epoch: 14 || iter 1048 || Loss: 2.4388 ||timer: 0.3322 sec.\n",
      "Epoch: 14 || iter 1056 || Loss: 2.2019 ||timer: 0.3331 sec.\n",
      "Epoch: 14 || iter 1064 || Loss: 1.9404 ||timer: 0.3306 sec.\n",
      "Epoch: 14 || iter 1072 || Loss: 2.2832 ||timer: 0.3298 sec.\n",
      "Epoch: 14 || iter 1080 || Loss: 2.3581 ||timer: 0.3299 sec.\n",
      "Epoch: 14 || iter 1088 || Loss: 2.0996 ||timer: 0.3627 sec.\n",
      "Epoch: 15 || iter 1096 || Loss: 2.2026 ||timer: 0.3339 sec.\n",
      "Epoch: 15 || iter 1104 || Loss: 2.1697 ||timer: 0.3333 sec.\n",
      "Epoch: 15 || iter 1112 || Loss: 2.3518 ||timer: 0.3315 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 || iter 1120 || Loss: 2.1590 ||timer: 0.3312 sec.\n",
      "Epoch: 15 || iter 1128 || Loss: 1.9565 ||timer: 0.3308 sec.\n",
      "Epoch: 15 || iter 1136 || Loss: 1.9776 ||timer: 0.3325 sec.\n",
      "Epoch: 15 || iter 1144 || Loss: 2.2676 ||timer: 0.3317 sec.\n",
      "Epoch: 15 || iter 1152 || Loss: 2.5384 ||timer: 0.3310 sec.\n",
      "Epoch: 15 || iter 1160 || Loss: 2.0623 ||timer: 0.1209 sec.\n",
      "Epoch: 15 || iter 1168 || Loss: 2.5343 ||Change learning rate to:  1e-05\n",
      "timer: 0.3320 sec.\n",
      "Epoch: 16 || iter 1176 || Loss: 2.4051 ||timer: 0.3322 sec.\n",
      "Epoch: 16 || iter 1184 || Loss: 1.9676 ||timer: 0.3311 sec.\n",
      "Epoch: 16 || iter 1192 || Loss: 2.2463 ||timer: 0.3333 sec.\n",
      "Epoch: 16 || iter 1200 || Loss: 1.9794 ||timer: 0.3317 sec.\n",
      "Epoch: 16 || iter 1208 || Loss: 1.8608 ||timer: 0.3325 sec.\n",
      "Epoch: 16 || iter 1216 || Loss: 2.3733 ||timer: 0.3326 sec.\n",
      "Epoch: 16 || iter 1224 || Loss: 2.4651 ||timer: 0.3304 sec.\n",
      "Epoch: 16 || iter 1232 || Loss: 2.0457 ||timer: 0.3297 sec.\n",
      "Epoch: 16 || iter 1240 || Loss: 1.9615 ||timer: 0.3703 sec.\n",
      "Epoch: 17 || iter 1248 || Loss: 2.0029 ||timer: 0.3341 sec.\n",
      "Epoch: 17 || iter 1256 || Loss: 2.0253 ||timer: 0.3308 sec.\n",
      "Epoch: 17 || iter 1264 || Loss: 1.7915 ||timer: 0.3305 sec.\n",
      "Epoch: 17 || iter 1272 || Loss: 2.4326 ||timer: 0.3315 sec.\n",
      "Epoch: 17 || iter 1280 || Loss: 2.0261 ||timer: 0.3320 sec.\n",
      "Epoch: 17 || iter 1288 || Loss: 2.4235 ||timer: 0.3329 sec.\n",
      "Epoch: 17 || iter 1296 || Loss: 1.8579 ||timer: 0.3304 sec.\n",
      "Epoch: 17 || iter 1304 || Loss: 2.4253 ||timer: 0.3307 sec.\n",
      "Epoch: 17 || iter 1312 || Loss: 2.0968 ||timer: 0.3334 sec.\n",
      "Epoch: 18 || iter 1320 || Loss: 2.2324 ||timer: 0.3306 sec.\n",
      "Epoch: 18 || iter 1328 || Loss: 2.0112 ||timer: 0.3315 sec.\n",
      "Epoch: 18 || iter 1336 || Loss: 2.3053 ||timer: 0.3315 sec.\n",
      "Epoch: 18 || iter 1344 || Loss: 2.2859 ||timer: 0.3331 sec.\n",
      "Epoch: 18 || iter 1352 || Loss: 2.5071 ||timer: 0.3312 sec.\n",
      "Epoch: 18 || iter 1360 || Loss: 2.3414 ||timer: 0.3322 sec.\n",
      "Epoch: 18 || iter 1368 || Loss: 2.0684 ||timer: 0.3306 sec.\n",
      "Epoch: 18 || iter 1376 || Loss: 2.1708 ||timer: 0.3294 sec.\n",
      "Epoch: 18 || iter 1384 || Loss: 2.2641 ||timer: 0.3270 sec.\n",
      "Epoch: 19 || iter 1392 || Loss: 2.1209 ||timer: 0.3324 sec.\n",
      "Epoch: 19 || iter 1400 || Loss: 2.3869 ||timer: 0.3328 sec.\n",
      "Epoch: 19 || iter 1408 || Loss: 2.2665 ||timer: 0.3405 sec.\n",
      "Epoch: 19 || iter 1416 || Loss: 1.9050 ||timer: 0.3351 sec.\n",
      "Epoch: 19 || iter 1424 || Loss: 2.4328 ||timer: 0.3324 sec.\n",
      "Epoch: 19 || iter 1432 || Loss: 2.0388 ||timer: 0.3337 sec.\n",
      "Epoch: 19 || iter 1440 || Loss: 1.9721 ||timer: 0.3307 sec.\n",
      "Epoch: 19 || iter 1448 || Loss: 2.4575 ||timer: 0.3309 sec.\n",
      "Epoch: 19 || iter 1456 || Loss: 2.1490 ||timer: 0.3281 sec.\n",
      "Epoch: 20 || iter 1464 || Loss: 2.2993 ||timer: 0.3325 sec.\n",
      "Epoch: 20 || iter 1472 || Loss: 2.0982 ||timer: 0.3313 sec.\n",
      "Epoch: 20 || iter 1480 || Loss: 2.1068 ||timer: 0.3313 sec.\n",
      "Epoch: 20 || iter 1488 || Loss: 2.5127 ||timer: 0.3305 sec.\n",
      "Epoch: 20 || iter 1496 || Loss: 1.8771 ||timer: 0.3326 sec.\n",
      "Epoch: 20 || iter 1504 || Loss: 1.9725 ||timer: 0.3360 sec.\n",
      "Epoch: 20 || iter 1512 || Loss: 1.8358 ||timer: 0.3306 sec.\n",
      "Epoch: 20 || iter 1520 || Loss: 1.8838 ||timer: 0.3305 sec.\n",
      "Epoch: 20 || iter 1528 || Loss: 2.2071 ||Saving state, Epoch: 20\n",
      "timer: 0.3184 sec.\n",
      "Epoch: 21 || iter 1536 || Loss: 2.0416 ||timer: 0.3321 sec.\n",
      "Epoch: 21 || iter 1544 || Loss: 1.9567 ||timer: 0.3312 sec.\n",
      "Epoch: 21 || iter 1552 || Loss: 2.1507 ||timer: 0.3326 sec.\n",
      "Epoch: 21 || iter 1560 || Loss: 2.2978 ||timer: 0.3313 sec.\n",
      "Epoch: 21 || iter 1568 || Loss: 2.1413 ||timer: 0.3334 sec.\n",
      "Epoch: 21 || iter 1576 || Loss: 2.1170 ||timer: 0.3319 sec.\n",
      "Epoch: 21 || iter 1584 || Loss: 3.0850 ||timer: 0.3311 sec.\n",
      "Epoch: 21 || iter 1592 || Loss: 1.9958 ||timer: 0.3309 sec.\n",
      "Epoch: 21 || iter 1600 || Loss: 1.6485 ||timer: 0.3469 sec.\n",
      "Epoch: 22 || iter 1608 || Loss: 2.4366 ||timer: 0.3359 sec.\n",
      "Epoch: 22 || iter 1616 || Loss: 1.9901 ||timer: 0.3304 sec.\n",
      "Epoch: 22 || iter 1624 || Loss: 2.3275 ||timer: 0.3326 sec.\n",
      "Epoch: 22 || iter 1632 || Loss: 1.9531 ||timer: 0.3328 sec.\n",
      "Epoch: 22 || iter 1640 || Loss: 1.7972 ||timer: 0.3326 sec.\n",
      "Epoch: 22 || iter 1648 || Loss: 2.3234 ||timer: 0.3333 sec.\n",
      "Epoch: 22 || iter 1656 || Loss: 1.7949 ||timer: 0.3294 sec.\n",
      "Epoch: 22 || iter 1664 || Loss: 2.1347 ||timer: 0.3294 sec.\n",
      "Epoch: 22 || iter 1672 || Loss: 1.8493 ||timer: 0.3566 sec.\n",
      "Epoch: 23 || iter 1680 || Loss: 1.6867 ||timer: 0.3867 sec.\n",
      "Epoch: 23 || iter 1688 || Loss: 2.2629 ||timer: 0.3313 sec.\n",
      "Epoch: 23 || iter 1696 || Loss: 1.7620 ||timer: 0.3354 sec.\n",
      "Epoch: 23 || iter 1704 || Loss: 1.8314 ||timer: 0.3328 sec.\n",
      "Epoch: 23 || iter 1712 || Loss: 2.0494 ||timer: 0.3325 sec.\n",
      "Epoch: 23 || iter 1720 || Loss: 2.3614 ||timer: 0.3329 sec.\n",
      "Epoch: 23 || iter 1728 || Loss: 2.1247 ||timer: 0.3307 sec.\n",
      "Epoch: 23 || iter 1736 || Loss: 1.8239 ||timer: 0.3305 sec.\n",
      "Epoch: 23 || iter 1744 || Loss: 2.2991 ||timer: 0.1222 sec.\n",
      "Epoch: 23 || iter 1752 || Loss: 1.9927 ||timer: 0.3314 sec.\n",
      "Epoch: 24 || iter 1760 || Loss: 1.9356 ||timer: 0.3341 sec.\n",
      "Epoch: 24 || iter 1768 || Loss: 1.9633 ||timer: 0.3330 sec.\n",
      "Epoch: 24 || iter 1776 || Loss: 1.9891 ||timer: 0.3323 sec.\n",
      "Epoch: 24 || iter 1784 || Loss: 1.9792 ||timer: 0.3311 sec.\n",
      "Epoch: 24 || iter 1792 || Loss: 1.9660 ||timer: 0.3329 sec.\n",
      "Epoch: 24 || iter 1800 || Loss: 2.1546 ||timer: 0.3733 sec.\n",
      "Epoch: 24 || iter 1808 || Loss: 2.1508 ||timer: 0.3307 sec.\n",
      "Epoch: 24 || iter 1816 || Loss: 1.8540 ||timer: 0.3314 sec.\n",
      "Epoch: 24 || iter 1824 || Loss: 2.3190 ||timer: 0.3358 sec.\n",
      "Epoch: 25 || iter 1832 || Loss: 1.7884 ||timer: 0.3346 sec.\n",
      "Epoch: 25 || iter 1840 || Loss: 1.9520 ||timer: 0.3333 sec.\n",
      "Epoch: 25 || iter 1848 || Loss: 1.9399 ||timer: 0.3314 sec.\n",
      "Epoch: 25 || iter 1856 || Loss: 1.8555 ||timer: 0.3304 sec.\n",
      "Epoch: 25 || iter 1864 || Loss: 2.0152 ||timer: 0.3332 sec.\n",
      "Epoch: 25 || iter 1872 || Loss: 2.5036 ||timer: 0.3326 sec.\n",
      "Epoch: 25 || iter 1880 || Loss: 1.7721 ||timer: 0.3306 sec.\n",
      "Epoch: 25 || iter 1888 || Loss: 1.9268 ||timer: 0.3309 sec.\n",
      "Epoch: 25 || iter 1896 || Loss: 1.8024 ||timer: 0.3317 sec.\n",
      "Epoch: 26 || iter 1904 || Loss: 1.9887 ||timer: 0.3334 sec.\n",
      "Epoch: 26 || iter 1912 || Loss: 1.9875 ||timer: 0.3326 sec.\n",
      "Epoch: 26 || iter 1920 || Loss: 2.0299 ||timer: 0.3330 sec.\n",
      "Epoch: 26 || iter 1928 || Loss: 2.3414 ||timer: 0.3316 sec.\n",
      "Epoch: 26 || iter 1936 || Loss: 2.0490 ||timer: 0.3311 sec.\n",
      "Epoch: 26 || iter 1944 || Loss: 2.1640 ||timer: 0.3319 sec.\n",
      "Epoch: 26 || iter 1952 || Loss: 2.4359 ||timer: 0.3323 sec.\n",
      "Epoch: 26 || iter 1960 || Loss: 2.4479 ||timer: 0.3317 sec.\n",
      "Epoch: 26 || iter 1968 || Loss: 2.5175 ||timer: 0.3321 sec.\n",
      "Epoch: 27 || iter 1976 || Loss: 2.0272 ||timer: 0.3320 sec.\n",
      "Epoch: 27 || iter 1984 || Loss: 2.2215 ||timer: 0.3328 sec.\n",
      "Epoch: 27 || iter 1992 || Loss: 1.9446 ||timer: 0.3321 sec.\n",
      "Epoch: 27 || iter 2000 || Loss: 1.9933 ||timer: 0.3325 sec.\n",
      "Epoch: 27 || iter 2008 || Loss: 2.1636 ||timer: 0.3316 sec.\n",
      "Epoch: 27 || iter 2016 || Loss: 2.0260 ||timer: 0.3314 sec.\n",
      "Epoch: 27 || iter 2024 || Loss: 1.8211 ||timer: 0.3298 sec.\n",
      "Epoch: 27 || iter 2032 || Loss: 1.7900 ||timer: 0.3302 sec.\n",
      "Epoch: 27 || iter 2040 || Loss: 1.9000 ||timer: 0.3243 sec.\n",
      "Epoch: 28 || iter 2048 || Loss: 1.8802 ||timer: 0.3339 sec.\n",
      "Epoch: 28 || iter 2056 || Loss: 1.9830 ||timer: 0.3332 sec.\n",
      "Epoch: 28 || iter 2064 || Loss: 2.2147 ||timer: 0.3319 sec.\n",
      "Epoch: 28 || iter 2072 || Loss: 2.3584 ||timer: 0.3323 sec.\n",
      "Epoch: 28 || iter 2080 || Loss: 1.8886 ||timer: 0.3322 sec.\n",
      "Epoch: 28 || iter 2088 || Loss: 2.6677 ||timer: 0.3323 sec.\n",
      "Epoch: 28 || iter 2096 || Loss: 2.6585 ||timer: 0.3312 sec.\n",
      "Epoch: 28 || iter 2104 || Loss: 2.0565 ||timer: 0.3312 sec.\n",
      "Epoch: 28 || iter 2112 || Loss: 1.9116 ||timer: 0.3278 sec.\n",
      "Epoch: 29 || iter 2120 || Loss: 2.3086 ||timer: 0.3325 sec.\n",
      "Epoch: 29 || iter 2128 || Loss: 2.1717 ||timer: 0.3341 sec.\n",
      "Epoch: 29 || iter 2136 || Loss: 2.1638 ||timer: 0.3316 sec.\n",
      "Epoch: 29 || iter 2144 || Loss: 1.9639 ||timer: 0.3333 sec.\n",
      "Epoch: 29 || iter 2152 || Loss: 2.2708 ||timer: 0.3340 sec.\n",
      "Epoch: 29 || iter 2160 || Loss: 1.6002 ||timer: 0.3328 sec.\n",
      "Epoch: 29 || iter 2168 || Loss: 2.0415 ||timer: 0.3304 sec.\n",
      "Epoch: 29 || iter 2176 || Loss: 2.0294 ||timer: 0.3306 sec.\n",
      "Epoch: 29 || iter 2184 || Loss: 1.7062 ||timer: 0.3286 sec.\n",
      "Epoch: 30 || iter 2192 || Loss: 1.7120 ||timer: 0.3335 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 || iter 2200 || Loss: 1.9727 ||timer: 0.3314 sec.\n",
      "Epoch: 30 || iter 2208 || Loss: 1.7055 ||timer: 0.3333 sec.\n",
      "Epoch: 30 || iter 2216 || Loss: 1.8683 ||timer: 0.3337 sec.\n",
      "Epoch: 30 || iter 2224 || Loss: 2.1711 ||timer: 0.3332 sec.\n",
      "Epoch: 30 || iter 2232 || Loss: 2.0588 ||timer: 0.3325 sec.\n",
      "Epoch: 30 || iter 2240 || Loss: 1.9905 ||timer: 0.3306 sec.\n",
      "Epoch: 30 || iter 2248 || Loss: 1.9505 ||timer: 0.3294 sec.\n",
      "Epoch: 30 || iter 2256 || Loss: 2.0431 ||Saving state, Epoch: 30\n",
      "Change learning rate to:  1e-05\n",
      "timer: 0.3784 sec.\n",
      "Epoch: 31 || iter 2264 || Loss: 2.2902 ||timer: 0.3307 sec.\n",
      "Epoch: 31 || iter 2272 || Loss: 2.0899 ||timer: 0.3329 sec.\n",
      "Epoch: 31 || iter 2280 || Loss: 1.9905 ||timer: 0.3374 sec.\n",
      "Epoch: 31 || iter 2288 || Loss: 1.7919 ||timer: 0.3411 sec.\n",
      "Epoch: 31 || iter 2296 || Loss: 2.5646 ||timer: 0.3343 sec.\n",
      "Epoch: 31 || iter 2304 || Loss: 2.3755 ||timer: 0.3331 sec.\n",
      "Epoch: 31 || iter 2312 || Loss: 1.7657 ||timer: 0.3304 sec.\n",
      "Epoch: 31 || iter 2320 || Loss: 2.7140 ||timer: 0.3312 sec.\n",
      "Epoch: 31 || iter 2328 || Loss: 2.0665 ||timer: 0.1208 sec.\n",
      "Epoch: 31 || iter 2336 || Loss: 1.8607 ||timer: 0.3349 sec.\n",
      "Epoch: 32 || iter 2344 || Loss: 2.0123 ||timer: 0.3325 sec.\n",
      "Epoch: 32 || iter 2352 || Loss: 1.9189 ||timer: 0.3353 sec.\n",
      "Epoch: 32 || iter 2360 || Loss: 1.8467 ||timer: 0.3323 sec.\n",
      "Epoch: 32 || iter 2368 || Loss: 1.5688 ||timer: 0.3310 sec.\n",
      "Epoch: 32 || iter 2376 || Loss: 2.1197 ||timer: 0.3315 sec.\n",
      "Epoch: 32 || iter 2384 || Loss: 1.8080 ||timer: 0.3327 sec.\n",
      "Epoch: 32 || iter 2392 || Loss: 1.9798 ||timer: 0.3310 sec.\n",
      "Epoch: 32 || iter 2400 || Loss: 1.9041 ||timer: 0.3318 sec.\n",
      "Epoch: 32 || iter 2408 || Loss: 1.7226 ||timer: 0.3319 sec.\n",
      "Epoch: 33 || iter 2416 || Loss: 2.1727 ||timer: 0.3322 sec.\n",
      "Epoch: 33 || iter 2424 || Loss: 1.7913 ||timer: 0.3315 sec.\n",
      "Epoch: 33 || iter 2432 || Loss: 2.3795 ||timer: 0.3305 sec.\n",
      "Epoch: 33 || iter 2440 || Loss: 1.5506 ||timer: 0.3331 sec.\n",
      "Epoch: 33 || iter 2448 || Loss: 1.9246 ||timer: 0.3329 sec.\n",
      "Epoch: 33 || iter 2456 || Loss: 1.7938 ||timer: 0.3319 sec.\n",
      "Epoch: 33 || iter 2464 || Loss: 2.2955 ||timer: 0.3309 sec.\n",
      "Epoch: 33 || iter 2472 || Loss: 2.5782 ||timer: 0.3303 sec.\n",
      "Epoch: 33 || iter 2480 || Loss: 2.7050 ||timer: 0.3319 sec.\n",
      "Epoch: 34 || iter 2488 || Loss: 1.9678 ||timer: 0.3318 sec.\n",
      "Epoch: 34 || iter 2496 || Loss: 2.2991 ||timer: 0.3357 sec.\n",
      "Epoch: 34 || iter 2504 || Loss: 1.8400 ||timer: 0.3321 sec.\n",
      "Epoch: 34 || iter 2512 || Loss: 1.7042 ||timer: 0.3322 sec.\n",
      "Epoch: 34 || iter 2520 || Loss: 1.8587 ||timer: 0.3327 sec.\n",
      "Epoch: 34 || iter 2528 || Loss: 2.0127 ||timer: 0.3324 sec.\n",
      "Epoch: 34 || iter 2536 || Loss: 2.8638 ||timer: 0.3312 sec.\n",
      "Epoch: 34 || iter 2544 || Loss: 1.8255 ||timer: 0.3303 sec.\n",
      "Epoch: 34 || iter 2552 || Loss: 1.9993 ||timer: 0.3283 sec.\n",
      "Epoch: 35 || iter 2560 || Loss: 1.8182 ||timer: 0.3342 sec.\n",
      "Epoch: 35 || iter 2568 || Loss: 2.2097 ||timer: 0.3314 sec.\n",
      "Epoch: 35 || iter 2576 || Loss: 1.9283 ||timer: 0.3326 sec.\n",
      "Epoch: 35 || iter 2584 || Loss: 1.7781 ||timer: 0.3308 sec.\n",
      "Epoch: 35 || iter 2592 || Loss: 2.1464 ||timer: 0.3336 sec.\n",
      "Epoch: 35 || iter 2600 || Loss: 2.1754 ||timer: 0.3324 sec.\n",
      "Epoch: 35 || iter 2608 || Loss: 2.2774 ||timer: 0.3314 sec.\n",
      "Epoch: 35 || iter 2616 || Loss: 1.7889 ||timer: 0.3314 sec.\n",
      "Epoch: 35 || iter 2624 || Loss: 1.8906 ||timer: 0.3242 sec.\n",
      "Epoch: 36 || iter 2632 || Loss: 1.8906 ||timer: 0.3342 sec.\n",
      "Epoch: 36 || iter 2640 || Loss: 2.1156 ||timer: 0.3334 sec.\n",
      "Epoch: 36 || iter 2648 || Loss: 2.0061 ||timer: 0.3590 sec.\n",
      "Epoch: 36 || iter 2656 || Loss: 2.2610 ||timer: 0.3328 sec.\n",
      "Epoch: 36 || iter 2664 || Loss: 1.7010 ||timer: 0.3695 sec.\n",
      "Epoch: 36 || iter 2672 || Loss: 2.5312 ||timer: 0.3728 sec.\n",
      "Epoch: 36 || iter 2680 || Loss: 1.9265 ||timer: 0.3309 sec.\n",
      "Epoch: 36 || iter 2688 || Loss: 1.8995 ||timer: 0.3306 sec.\n",
      "Epoch: 36 || iter 2696 || Loss: 1.9060 ||timer: 0.3190 sec.\n",
      "Epoch: 37 || iter 2704 || Loss: 2.1137 ||timer: 0.3336 sec.\n",
      "Epoch: 37 || iter 2712 || Loss: 2.4413 ||timer: 0.3348 sec.\n",
      "Epoch: 37 || iter 2720 || Loss: 2.0127 ||timer: 0.3333 sec.\n",
      "Epoch: 37 || iter 2728 || Loss: 1.8696 ||timer: 0.3326 sec.\n",
      "Epoch: 37 || iter 2736 || Loss: 2.3581 ||timer: 0.3307 sec.\n",
      "Epoch: 37 || iter 2744 || Loss: 1.8363 ||timer: 0.3335 sec.\n",
      "Epoch: 37 || iter 2752 || Loss: 1.8429 ||timer: 0.3322 sec.\n",
      "Epoch: 37 || iter 2760 || Loss: 2.0115 ||timer: 0.3305 sec.\n",
      "Epoch: 37 || iter 2768 || Loss: 1.8595 ||timer: 0.3528 sec.\n",
      "Epoch: 38 || iter 2776 || Loss: 1.8404 ||timer: 0.3320 sec.\n",
      "Epoch: 38 || iter 2784 || Loss: 1.7510 ||timer: 0.3391 sec.\n",
      "Epoch: 38 || iter 2792 || Loss: 1.5984 ||timer: 0.3372 sec.\n",
      "Epoch: 38 || iter 2800 || Loss: 1.8506 ||timer: 0.3329 sec.\n",
      "Epoch: 38 || iter 2808 || Loss: 2.5250 ||timer: 0.3320 sec.\n",
      "Epoch: 38 || iter 2816 || Loss: 2.0819 ||timer: 0.3317 sec.\n",
      "Epoch: 38 || iter 2824 || Loss: 2.2425 ||timer: 0.3306 sec.\n",
      "Epoch: 38 || iter 2832 || Loss: 1.8512 ||timer: 0.3318 sec.\n",
      "Epoch: 38 || iter 2840 || Loss: 1.9975 ||timer: 0.3580 sec.\n",
      "Epoch: 39 || iter 2848 || Loss: 1.9177 ||timer: 0.3313 sec.\n",
      "Epoch: 39 || iter 2856 || Loss: 1.6395 ||timer: 0.3322 sec.\n",
      "Epoch: 39 || iter 2864 || Loss: 2.0022 ||timer: 0.3314 sec.\n",
      "Epoch: 39 || iter 2872 || Loss: 1.9402 ||timer: 0.3336 sec.\n",
      "Epoch: 39 || iter 2880 || Loss: 2.2803 ||timer: 0.3328 sec.\n",
      "Epoch: 39 || iter 2888 || Loss: 2.1586 ||timer: 0.3337 sec.\n",
      "Epoch: 39 || iter 2896 || Loss: 2.0520 ||timer: 0.3309 sec.\n",
      "Epoch: 39 || iter 2904 || Loss: 2.0161 ||timer: 0.3298 sec.\n",
      "Epoch: 39 || iter 2912 || Loss: 2.0057 ||timer: 0.1208 sec.\n",
      "Epoch: 39 || iter 2920 || Loss: 1.4494 ||timer: 0.3312 sec.\n",
      "Epoch: 40 || iter 2928 || Loss: 1.8700 ||timer: 0.3333 sec.\n",
      "Epoch: 40 || iter 2936 || Loss: 1.9212 ||timer: 0.3330 sec.\n",
      "Epoch: 40 || iter 2944 || Loss: 1.6414 ||timer: 0.3312 sec.\n",
      "Epoch: 40 || iter 2952 || Loss: 2.0397 ||timer: 0.3308 sec.\n",
      "Epoch: 40 || iter 2960 || Loss: 1.7184 ||timer: 0.3326 sec.\n",
      "Epoch: 40 || iter 2968 || Loss: 1.7205 ||timer: 0.3331 sec.\n",
      "Epoch: 40 || iter 2976 || Loss: 1.9137 ||timer: 0.3309 sec.\n",
      "Epoch: 40 || iter 2984 || Loss: 2.0964 ||timer: 0.3310 sec.\n",
      "Epoch: 40 || iter 2992 || Loss: 1.8707 ||Saving state, Epoch: 40\n",
      "timer: 0.3299 sec.\n",
      "Epoch: 41 || iter 3000 || Loss: 2.0631 ||timer: 0.3297 sec.\n",
      "Epoch: 41 || iter 3008 || Loss: 1.9599 ||timer: 0.3324 sec.\n",
      "Epoch: 41 || iter 3016 || Loss: 1.9133 ||timer: 0.3336 sec.\n",
      "Epoch: 41 || iter 3024 || Loss: 1.8584 ||timer: 0.3408 sec.\n",
      "Epoch: 41 || iter 3032 || Loss: 1.9999 ||timer: 0.3331 sec.\n",
      "Epoch: 41 || iter 3040 || Loss: 1.9909 ||timer: 0.3319 sec.\n",
      "Epoch: 41 || iter 3048 || Loss: 1.7416 ||timer: 0.3295 sec.\n",
      "Epoch: 41 || iter 3056 || Loss: 1.8371 ||timer: 0.3315 sec.\n",
      "Epoch: 41 || iter 3064 || Loss: 1.9224 ||timer: 0.3334 sec.\n",
      "Epoch: 42 || iter 3072 || Loss: 2.0241 ||timer: 0.3313 sec.\n",
      "Epoch: 42 || iter 3080 || Loss: 2.2229 ||timer: 0.3330 sec.\n",
      "Epoch: 42 || iter 3088 || Loss: 2.0579 ||timer: 0.3349 sec.\n",
      "Epoch: 42 || iter 3096 || Loss: 1.7760 ||timer: 0.3299 sec.\n",
      "Epoch: 42 || iter 3104 || Loss: 1.7415 ||timer: 0.3299 sec.\n",
      "Epoch: 42 || iter 3112 || Loss: 2.1814 ||timer: 0.3318 sec.\n",
      "Epoch: 42 || iter 3120 || Loss: 1.8940 ||timer: 0.3316 sec.\n",
      "Epoch: 42 || iter 3128 || Loss: 2.0208 ||timer: 0.3305 sec.\n",
      "Epoch: 42 || iter 3136 || Loss: 1.6873 ||timer: 0.3339 sec.\n",
      "Epoch: 43 || iter 3144 || Loss: 2.0833 ||timer: 0.3407 sec.\n",
      "Epoch: 43 || iter 3152 || Loss: 1.8895 ||timer: 0.3308 sec.\n",
      "Epoch: 43 || iter 3160 || Loss: 2.2315 ||timer: 0.3327 sec.\n",
      "Epoch: 43 || iter 3168 || Loss: 1.8710 ||timer: 0.3324 sec.\n",
      "Epoch: 43 || iter 3176 || Loss: 2.0014 ||timer: 0.3313 sec.\n",
      "Epoch: 43 || iter 3184 || Loss: 2.3048 ||timer: 0.3339 sec.\n",
      "Epoch: 43 || iter 3192 || Loss: 2.3507 ||timer: 0.3302 sec.\n",
      "Epoch: 43 || iter 3200 || Loss: 1.7489 ||timer: 0.3301 sec.\n",
      "Epoch: 43 || iter 3208 || Loss: 2.1735 ||timer: 0.3259 sec.\n",
      "Epoch: 44 || iter 3216 || Loss: 1.8270 ||timer: 0.3342 sec.\n",
      "Epoch: 44 || iter 3224 || Loss: 1.9906 ||timer: 0.3327 sec.\n",
      "Epoch: 44 || iter 3232 || Loss: 2.0527 ||timer: 0.3326 sec.\n",
      "Epoch: 44 || iter 3240 || Loss: 2.5154 ||timer: 0.3705 sec.\n",
      "Epoch: 44 || iter 3248 || Loss: 2.0103 ||timer: 0.3400 sec.\n",
      "Epoch: 44 || iter 3256 || Loss: 2.0235 ||timer: 0.3314 sec.\n",
      "Epoch: 44 || iter 3264 || Loss: 2.2333 ||timer: 0.3313 sec.\n",
      "Epoch: 44 || iter 3272 || Loss: 1.7586 ||timer: 0.3323 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 || iter 3280 || Loss: 1.8688 ||timer: 0.3247 sec.\n",
      "Epoch: 45 || iter 3288 || Loss: 1.8136 ||timer: 0.3328 sec.\n",
      "Epoch: 45 || iter 3296 || Loss: 1.7519 ||timer: 0.3325 sec.\n",
      "Epoch: 45 || iter 3304 || Loss: 2.0019 ||timer: 0.3335 sec.\n",
      "Epoch: 45 || iter 3312 || Loss: 2.3339 ||timer: 0.3317 sec.\n",
      "Epoch: 45 || iter 3320 || Loss: 1.8351 ||timer: 0.3328 sec.\n",
      "Epoch: 45 || iter 3328 || Loss: 2.1842 ||timer: 0.3308 sec.\n",
      "Epoch: 45 || iter 3336 || Loss: 1.9191 ||timer: 0.3315 sec.\n",
      "Epoch: 45 || iter 3344 || Loss: 1.7038 ||timer: 0.3310 sec.\n",
      "Epoch: 45 || iter 3352 || Loss: 1.7539 ||Change learning rate to:  1e-05\n",
      "timer: 0.3858 sec.\n",
      "Epoch: 46 || iter 3360 || Loss: 1.9890 ||timer: 0.3330 sec.\n",
      "Epoch: 46 || iter 3368 || Loss: 2.1202 ||timer: 0.3321 sec.\n",
      "Epoch: 46 || iter 3376 || Loss: 1.8326 ||timer: 0.3537 sec.\n",
      "Epoch: 46 || iter 3384 || Loss: 1.7577 ||timer: 0.3336 sec.\n",
      "Epoch: 46 || iter 3392 || Loss: 1.7380 ||timer: 0.3319 sec.\n",
      "Epoch: 46 || iter 3400 || Loss: 1.9824 ||timer: 0.3352 sec.\n",
      "Epoch: 46 || iter 3408 || Loss: 1.9586 ||timer: 0.3301 sec.\n",
      "Epoch: 46 || iter 3416 || Loss: 1.8654 ||timer: 0.3296 sec.\n",
      "Epoch: 46 || iter 3424 || Loss: 1.9440 ||timer: 0.4205 sec.\n",
      "Epoch: 47 || iter 3432 || Loss: 2.0949 ||timer: 0.3317 sec.\n",
      "Epoch: 47 || iter 3440 || Loss: 1.8968 ||timer: 0.3360 sec.\n",
      "Epoch: 47 || iter 3448 || Loss: 1.7537 ||timer: 0.3427 sec.\n",
      "Epoch: 47 || iter 3456 || Loss: 1.9317 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-02bbf7526157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mconf_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        iteration += 1\n",
    "        images, targets = batch\n",
    "        if CUDA:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "\n",
    "        # Forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.item()\n",
    "        conf_loss += loss_c.item()\n",
    "\n",
    "        if iteration % 8 == 0:\n",
    "                print('timer: %.4f sec.' % (t1 - t0))\n",
    "                print('Epoch: ' + str(epoch) + ' || iter ' + repr(PRETRAINED_EPOCH + iteration) + ' || Loss: %.4f ||' % (loss.item()), end='')\n",
    "\n",
    "    if epoch != 0 and epoch % SAVE_MODEL_EPOCH == 0:\n",
    "        print('Saving state, Epoch:', epoch)\n",
    "        torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\n",
    "                    repr(PRETRAINED_EPOCH + epoch) + '.pth')\n",
    "    if epoch in adjust_lr_epoch:\n",
    "        adjust_learning_rate(optimizer, GAMMA, epoch)\n",
    "    # Save final model\n",
    "torch.save(ssd_net.state_dict(),\n",
    "        SAVE_FOLDER + DATASET_NAME + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''step_index = 0\n",
    "for iteration in range(START_ITER, cfg['max_iter']):\n",
    "    if iteration in cfg['lr_steps']:\n",
    "        step_index += 1\n",
    "        adjust_learning_rate(optimizer, GAMMA, step_index)\n",
    "    \n",
    "    # make sure data iter not out of range\n",
    "    try:\n",
    "        images, targets = next(batch_iterator)\n",
    "        #print(targets[0][0][4].item(), label[int(targets[0][0][4].item())])\n",
    "    except StopIteration:\n",
    "        batch_iterator = iter(data_loader)\n",
    "        images, targets = next(batch_iterator)\n",
    "    if CUDA:\n",
    "        images = Variable(images.cuda())\n",
    "        targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "    else:\n",
    "        images = Variable(images)\n",
    "        targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "    \n",
    "    # Forward\n",
    "    t0 = time.time()\n",
    "    out = net(images)\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_l, loss_c = criterion(out, targets)\n",
    "    loss = loss_l + loss_c\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "    loc_loss += loss_l.item()\n",
    "    conf_loss += loss_c.item()\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(PRETRAINED_ITER + iteration) + ' || Loss: %.4f ||' % (loss.item()), end='')\n",
    "    \n",
    "    if iteration != 0 and iteration % SAVE_MODEL_ITER == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\n",
    "                       repr(PRETRAINED_ITER + iteration) + '.pth')\n",
    "# Save final model\n",
    "torch.save(ssd_net.state_dict(),\n",
    "            SAVE_FOLDER + DATASET_NAME + '.pth')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
