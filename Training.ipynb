{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/media/arg_ws3/5E703E3A703E18EB/data/subt_real_ssd\"\n",
    "DATASET_NAME = \"subt_real_mask\"\n",
    "cfg = subt_real\n",
    "BASE_NET = \"./weights/vgg16_reducedfc.pth\"\n",
    "DATA_DETECTION = SUBTREALDetection\n",
    "BATCH_SIZE = 32\n",
    "#PRETRAINED_MODEL = \"/home/arg_ws3/ssd.pytorch/weights/person/person_67500.pth\"\n",
    "PRETRAINED_MODEL = None\n",
    "PRETRAINED_EPOCH = 0\n",
    "SAVE_MODEL_EPOCH = 5\n",
    "START_ITER = 0\n",
    "NUM_WORKERS = 4\n",
    "EPOCH = 100\n",
    "adjust_lr_epoch = [60, 80, 150]\n",
    "CUDA = True\n",
    "LR = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "GAMMA = 0.1\n",
    "VISDOM = False\n",
    "SAVE_FOLDER = \"./weights/\" + DATASET_NAME + \"/\"\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if not CUDA:\n",
    "        print(\"WTF are u wasting your CUDA device?\")\n",
    "    else:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "# Initial model weights & bias\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "# Adjust learning rate during training\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    #lr = LR * (gamma ** (step))\n",
    "    lr = LR * (gamma)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        print(\"Change learning rate to: \", lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index: \n",
      " {'bb_extinguisher': 0}\n",
      "['bb_extinguisher', 'None']\n"
     ]
    }
   ],
   "source": [
    "dataset = DATA_DETECTION(root=DATASET_ROOT, image_sets=['train'],transform=SSDAugmentation(cfg['min_dim'], MEANS))\n",
    "\n",
    "classes = dataset.target_transform.class_to_ind\n",
    "print(\"Class to index: \\n\", classes)\n",
    "classes = sorted(classes.items(), key=lambda kv: kv[1])\n",
    "label = []\n",
    "for i in classes:\n",
    "    label.append(i[0])\n",
    "label.append('None')\n",
    "print(label)\n",
    "true_label = ['person', 'palm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
      "/home/arg_ws3/ssd.pytorch/layers/modules/l2norm.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.weight,self.gamma)\n"
     ]
    }
   ],
   "source": [
    "ssd_pretrained = build_ssd('train', cfg['min_dim'], 2)\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_pretrained)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "    ssd_pretrained.load_weights(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Delcare SSD Network\n",
    "ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "net = ssd_net\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "SAME_CLASS = False\n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    if SAME_CLASS:\n",
    "        print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "        ssd_net.load_weights(PRETRAINED_MODEL)\n",
    "    else:\n",
    "        print('Load pretrained model with different classes')\n",
    "        ssd_pretrained = build_ssd('train', cfg['min_dim'], 2)\n",
    "        ssd_pretrained.load_weights(PRETRAINED_MODEL)\n",
    "        ssd_net.vgg = ssd_pretrained.vgg\n",
    "        ssd_net.extras = ssd_pretrained.extras\n",
    "        ssd_net.loc = ssd_pretrained.loc\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "else:\n",
    "    print('Initializing weights...')\n",
    "    vgg_weights = torch.load(BASE_NET) # load vgg pretrained model\n",
    "    ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "    ssd_net.extras.apply(weights_init) # Initial SSD model weights & bias\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM,\n",
    "                weight_decay=WEIGHT_DECAY)\n",
    "criterion = MultiBoxLoss(BATCH_SIZE ,cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                False, CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(list(ssd_net.parameters()))-30):\n",
    "#    list(ssd_net.parameters())[i].data = list(ssd_pretrained.parameters())[i]\n",
    "#list(ssd_net.conf)[-1] = list(ssd_pretrained.conf)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Training SSD on: subt_real_mask\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "# loss counters\n",
    "loc_loss = 0\n",
    "conf_loss = 0\n",
    "epoch = 0\n",
    "print('Loading the dataset...')\n",
    "epoch_size = len(dataset) // BATCH_SIZE\n",
    "print('Training SSD on:', DATASET_NAME)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, BATCH_SIZE,\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                shuffle=True, collate_fn=detection_collate,\n",
    "                                pin_memory=True)\n",
    "batch_iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 0.3830 sec.\n",
      "Epoch: 0 || iter 8 || Loss: 8.0443 ||timer: 0.3640 sec.\n",
      "Epoch: 1 || iter 16 || Loss: 7.0632 ||timer: 0.3879 sec.\n",
      "Epoch: 1 || iter 24 || Loss: 5.8559 ||timer: 0.3816 sec.\n",
      "Epoch: 2 || iter 32 || Loss: 5.7918 ||timer: 0.4285 sec.\n",
      "Epoch: 3 || iter 40 || Loss: 5.4369 ||timer: 0.3939 sec.\n",
      "Epoch: 3 || iter 48 || Loss: 5.4252 ||timer: 0.3852 sec.\n",
      "Epoch: 4 || iter 56 || Loss: 4.8923 ||timer: 0.3823 sec.\n",
      "Epoch: 4 || iter 64 || Loss: 5.0082 ||timer: 0.3738 sec.\n",
      "Epoch: 5 || iter 72 || Loss: 4.7462 ||Saving state, Epoch: 5\n",
      "timer: 0.4475 sec.\n",
      "Epoch: 6 || iter 80 || Loss: 4.5241 ||timer: 0.3883 sec.\n",
      "Epoch: 6 || iter 88 || Loss: 4.8962 ||timer: 0.3907 sec.\n",
      "Epoch: 7 || iter 96 || Loss: 4.4047 ||timer: 0.3012 sec.\n",
      "Epoch: 7 || iter 104 || Loss: 4.0686 ||timer: 0.3859 sec.\n",
      "Epoch: 8 || iter 112 || Loss: 3.9913 ||timer: 0.3945 sec.\n",
      "Epoch: 9 || iter 120 || Loss: 4.3680 ||timer: 0.3839 sec.\n",
      "Epoch: 9 || iter 128 || Loss: 4.2780 ||timer: 0.3840 sec.\n",
      "Epoch: 10 || iter 136 || Loss: 4.1082 ||Saving state, Epoch: 10\n",
      "timer: 0.4118 sec.\n",
      "Epoch: 11 || iter 144 || Loss: 3.9560 ||timer: 0.3760 sec.\n",
      "Epoch: 11 || iter 152 || Loss: 3.6838 ||timer: 0.3918 sec.\n",
      "Epoch: 12 || iter 160 || Loss: 3.6807 ||timer: 0.3824 sec.\n",
      "Epoch: 12 || iter 168 || Loss: 3.6683 ||timer: 0.3777 sec.\n",
      "Epoch: 13 || iter 176 || Loss: 3.5540 ||timer: 0.3768 sec.\n",
      "Epoch: 14 || iter 184 || Loss: 3.7842 ||timer: 0.3882 sec.\n",
      "Epoch: 14 || iter 192 || Loss: 3.3144 ||timer: 0.3931 sec.\n",
      "Epoch: 15 || iter 200 || Loss: 3.2865 ||timer: 0.3058 sec.\n",
      "Epoch: 15 || iter 208 || Loss: 3.6505 ||Saving state, Epoch: 15\n",
      "timer: 0.3814 sec.\n",
      "Epoch: 16 || iter 216 || Loss: 3.3438 ||timer: 0.3899 sec.\n",
      "Epoch: 17 || iter 224 || Loss: 3.2279 ||timer: 0.3885 sec.\n",
      "Epoch: 17 || iter 232 || Loss: 2.9463 ||timer: 0.3835 sec.\n",
      "Epoch: 18 || iter 240 || Loss: 2.8685 ||timer: 0.3960 sec.\n",
      "Epoch: 19 || iter 248 || Loss: 2.8011 ||timer: 0.3911 sec.\n",
      "Epoch: 19 || iter 256 || Loss: 2.6307 ||timer: 0.3876 sec.\n",
      "Epoch: 20 || iter 264 || Loss: 2.4070 ||timer: 0.3797 sec.\n",
      "Epoch: 20 || iter 272 || Loss: 1.9931 ||Saving state, Epoch: 20\n",
      "timer: 0.3832 sec.\n",
      "Epoch: 21 || iter 280 || Loss: 2.3673 ||timer: 0.3754 sec.\n",
      "Epoch: 22 || iter 288 || Loss: 2.6952 ||timer: 0.3859 sec.\n",
      "Epoch: 22 || iter 296 || Loss: 2.2137 ||timer: 0.3827 sec.\n",
      "Epoch: 23 || iter 304 || Loss: 2.2654 ||timer: 0.3013 sec.\n",
      "Epoch: 23 || iter 312 || Loss: 2.9966 ||timer: 0.3815 sec.\n",
      "Epoch: 24 || iter 320 || Loss: 1.7800 ||timer: 0.3760 sec.\n",
      "Epoch: 25 || iter 328 || Loss: 1.9183 ||timer: 0.3841 sec.\n",
      "Epoch: 25 || iter 336 || Loss: 2.0312 ||Saving state, Epoch: 25\n",
      "timer: 0.3805 sec.\n",
      "Epoch: 26 || iter 344 || Loss: 1.7615 ||timer: 0.3972 sec.\n",
      "Epoch: 27 || iter 352 || Loss: 1.8315 ||timer: 0.3787 sec.\n",
      "Epoch: 27 || iter 360 || Loss: 1.8595 ||timer: 0.3805 sec.\n",
      "Epoch: 28 || iter 368 || Loss: 1.8101 ||timer: 0.3831 sec.\n",
      "Epoch: 28 || iter 376 || Loss: 1.9077 ||timer: 0.3774 sec.\n",
      "Epoch: 29 || iter 384 || Loss: 1.6464 ||timer: 0.3813 sec.\n",
      "Epoch: 30 || iter 392 || Loss: 2.0598 ||timer: 0.3797 sec.\n",
      "Epoch: 30 || iter 400 || Loss: 1.7214 ||Saving state, Epoch: 30\n",
      "timer: 0.3909 sec.\n",
      "Epoch: 31 || iter 408 || Loss: 2.2332 ||timer: 0.3051 sec.\n",
      "Epoch: 31 || iter 416 || Loss: 1.6566 ||timer: 0.3925 sec.\n",
      "Epoch: 32 || iter 424 || Loss: 1.4926 ||timer: 0.3883 sec.\n",
      "Epoch: 33 || iter 432 || Loss: 1.6154 ||timer: 0.3817 sec.\n",
      "Epoch: 33 || iter 440 || Loss: 1.6028 ||timer: 0.3847 sec.\n",
      "Epoch: 34 || iter 448 || Loss: 1.7231 ||timer: 0.3912 sec.\n",
      "Epoch: 35 || iter 456 || Loss: 1.5828 ||timer: 0.3788 sec.\n",
      "Epoch: 35 || iter 464 || Loss: 1.5575 ||Saving state, Epoch: 35\n",
      "timer: 0.3820 sec.\n",
      "Epoch: 36 || iter 472 || Loss: 2.1760 ||timer: 0.3767 sec.\n",
      "Epoch: 36 || iter 480 || Loss: 1.4847 ||timer: 0.3803 sec.\n",
      "Epoch: 37 || iter 488 || Loss: 1.6760 ||timer: 0.3740 sec.\n",
      "Epoch: 38 || iter 496 || Loss: 1.3472 ||timer: 0.3896 sec.\n",
      "Epoch: 38 || iter 504 || Loss: 1.8089 ||timer: 0.3901 sec.\n",
      "Epoch: 39 || iter 512 || Loss: 1.4867 ||timer: 0.3017 sec.\n",
      "Epoch: 39 || iter 520 || Loss: 1.2546 ||timer: 0.3831 sec.\n",
      "Epoch: 40 || iter 528 || Loss: 1.7488 ||Saving state, Epoch: 40\n",
      "timer: 0.3900 sec.\n",
      "Epoch: 41 || iter 536 || Loss: 1.5073 ||timer: 0.3862 sec.\n",
      "Epoch: 41 || iter 544 || Loss: 1.1344 ||timer: 0.3808 sec.\n",
      "Epoch: 42 || iter 552 || Loss: 1.4350 ||timer: 0.3959 sec.\n",
      "Epoch: 43 || iter 560 || Loss: 1.5535 ||timer: 0.3828 sec.\n",
      "Epoch: 43 || iter 568 || Loss: 1.5335 ||timer: 0.3871 sec.\n",
      "Epoch: 44 || iter 576 || Loss: 1.3231 ||timer: 0.3813 sec.\n",
      "Epoch: 44 || iter 584 || Loss: 1.5247 ||timer: 0.3785 sec.\n",
      "Epoch: 45 || iter 592 || Loss: 1.5494 ||Saving state, Epoch: 45\n",
      "timer: 0.3699 sec.\n",
      "Epoch: 46 || iter 600 || Loss: 1.4848 ||timer: 0.3795 sec.\n",
      "Epoch: 46 || iter 608 || Loss: 1.1145 ||timer: 0.3950 sec.\n",
      "Epoch: 47 || iter 616 || Loss: 1.1955 ||timer: 0.3024 sec.\n",
      "Epoch: 47 || iter 624 || Loss: 2.5151 ||timer: 0.3833 sec.\n",
      "Epoch: 48 || iter 632 || Loss: 1.5295 ||timer: 0.3886 sec.\n",
      "Epoch: 49 || iter 640 || Loss: 1.6695 ||timer: 0.3848 sec.\n",
      "Epoch: 49 || iter 648 || Loss: 1.3898 ||timer: 0.3864 sec.\n",
      "Epoch: 50 || iter 656 || Loss: 1.2323 ||Saving state, Epoch: 50\n",
      "timer: 0.3885 sec.\n",
      "Epoch: 51 || iter 664 || Loss: 1.1327 ||timer: 0.3789 sec.\n",
      "Epoch: 51 || iter 672 || Loss: 1.3828 ||timer: 0.3838 sec.\n",
      "Epoch: 52 || iter 680 || Loss: 1.0811 ||timer: 0.3826 sec.\n",
      "Epoch: 52 || iter 688 || Loss: 1.3546 ||timer: 0.3843 sec.\n",
      "Epoch: 53 || iter 696 || Loss: 1.4413 ||timer: 0.3759 sec.\n",
      "Epoch: 54 || iter 704 || Loss: 1.2694 ||timer: 0.3765 sec.\n",
      "Epoch: 54 || iter 712 || Loss: 1.2547 ||timer: 0.3891 sec.\n",
      "Epoch: 55 || iter 720 || Loss: 1.0950 ||timer: 0.3040 sec.\n",
      "Epoch: 55 || iter 728 || Loss: 1.2062 ||Saving state, Epoch: 55\n",
      "timer: 0.3836 sec.\n",
      "Epoch: 56 || iter 736 || Loss: 1.4233 ||timer: 0.3745 sec.\n",
      "Epoch: 57 || iter 744 || Loss: 1.2809 ||timer: 0.3886 sec.\n",
      "Epoch: 57 || iter 752 || Loss: 1.3125 ||timer: 0.3899 sec.\n",
      "Epoch: 58 || iter 760 || Loss: 1.1118 ||timer: 0.4371 sec.\n",
      "Epoch: 59 || iter 768 || Loss: 1.3344 ||timer: 0.3837 sec.\n",
      "Epoch: 59 || iter 776 || Loss: 1.6674 ||timer: 0.3821 sec.\n",
      "Epoch: 60 || iter 784 || Loss: 1.1304 ||timer: 0.3835 sec.\n",
      "Epoch: 60 || iter 792 || Loss: 1.3300 ||Saving state, Epoch: 60\n",
      "Change learning rate to:  1e-05\n",
      "timer: 0.3801 sec.\n",
      "Epoch: 61 || iter 800 || Loss: 1.1692 ||timer: 0.3898 sec.\n",
      "Epoch: 62 || iter 808 || Loss: 1.1081 ||timer: 0.3874 sec.\n",
      "Epoch: 62 || iter 816 || Loss: 1.2857 ||timer: 0.3840 sec.\n",
      "Epoch: 63 || iter 824 || Loss: 0.9771 ||timer: 0.2966 sec.\n",
      "Epoch: 63 || iter 832 || Loss: 1.0242 ||timer: 0.3801 sec.\n",
      "Epoch: 64 || iter 840 || Loss: 1.2333 ||timer: 0.3795 sec.\n",
      "Epoch: 65 || iter 848 || Loss: 1.3778 ||timer: 0.3758 sec.\n",
      "Epoch: 65 || iter 856 || Loss: 0.9927 ||Saving state, Epoch: 65\n",
      "timer: 0.3778 sec.\n",
      "Epoch: 66 || iter 864 || Loss: 1.1254 ||timer: 0.4087 sec.\n",
      "Epoch: 67 || iter 872 || Loss: 1.2293 ||timer: 0.3885 sec.\n",
      "Epoch: 67 || iter 880 || Loss: 1.1342 ||timer: 0.3897 sec.\n",
      "Epoch: 68 || iter 888 || Loss: 1.1552 ||timer: 0.3854 sec.\n",
      "Epoch: 68 || iter 896 || Loss: 0.9480 ||timer: 0.3798 sec.\n",
      "Epoch: 69 || iter 904 || Loss: 1.0731 ||timer: 0.3863 sec.\n",
      "Epoch: 70 || iter 912 || Loss: 0.9443 ||timer: 0.3831 sec.\n",
      "Epoch: 70 || iter 920 || Loss: 1.1480 ||Saving state, Epoch: 70\n",
      "timer: 0.3908 sec.\n",
      "Epoch: 71 || iter 928 || Loss: 1.0623 ||timer: 0.3014 sec.\n",
      "Epoch: 71 || iter 936 || Loss: 1.0942 ||timer: 0.3863 sec.\n",
      "Epoch: 72 || iter 944 || Loss: 0.9716 ||timer: 0.3953 sec.\n",
      "Epoch: 73 || iter 952 || Loss: 1.2357 ||timer: 0.3895 sec.\n",
      "Epoch: 73 || iter 960 || Loss: 1.0507 ||timer: 0.3881 sec.\n",
      "Epoch: 74 || iter 968 || Loss: 1.0366 ||timer: 0.4002 sec.\n",
      "Epoch: 75 || iter 976 || Loss: 0.9381 ||timer: 0.3874 sec.\n",
      "Epoch: 75 || iter 984 || Loss: 1.0455 ||Saving state, Epoch: 75\n",
      "timer: 0.3834 sec.\n",
      "Epoch: 76 || iter 992 || Loss: 1.0319 ||timer: 0.3820 sec.\n",
      "Epoch: 76 || iter 1000 || Loss: 1.1562 ||timer: 0.3828 sec.\n",
      "Epoch: 77 || iter 1008 || Loss: 1.2474 ||timer: 0.4053 sec.\n",
      "Epoch: 78 || iter 1016 || Loss: 1.2632 ||timer: 0.3941 sec.\n",
      "Epoch: 78 || iter 1024 || Loss: 0.9798 ||timer: 0.3795 sec.\n",
      "Epoch: 79 || iter 1032 || Loss: 1.0156 ||timer: 0.3011 sec.\n",
      "Epoch: 79 || iter 1040 || Loss: 0.9508 ||timer: 0.3802 sec.\n",
      "Epoch: 80 || iter 1048 || Loss: 0.8904 ||Saving state, Epoch: 80\n",
      "Change learning rate to:  1e-05\n",
      "timer: 0.3884 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 || iter 1056 || Loss: 0.8137 ||timer: 0.3919 sec.\n",
      "Epoch: 81 || iter 1064 || Loss: 1.3517 ||timer: 0.3880 sec.\n",
      "Epoch: 82 || iter 1072 || Loss: 0.9174 ||timer: 0.4260 sec.\n",
      "Epoch: 83 || iter 1080 || Loss: 1.4284 ||timer: 0.3841 sec.\n",
      "Epoch: 83 || iter 1088 || Loss: 1.3760 ||timer: 0.3861 sec.\n",
      "Epoch: 84 || iter 1096 || Loss: 1.2914 ||timer: 0.3848 sec.\n",
      "Epoch: 84 || iter 1104 || Loss: 1.1744 ||timer: 0.3836 sec.\n",
      "Epoch: 85 || iter 1112 || Loss: 0.9604 ||Saving state, Epoch: 85\n",
      "timer: 0.3855 sec.\n",
      "Epoch: 86 || iter 1120 || Loss: 0.9126 ||timer: 0.3740 sec.\n",
      "Epoch: 86 || iter 1128 || Loss: 0.9971 ||timer: 0.3974 sec.\n",
      "Epoch: 87 || iter 1136 || Loss: 1.2082 ||timer: 0.3072 sec.\n",
      "Epoch: 87 || iter 1144 || Loss: 0.8821 ||timer: 0.3832 sec.\n",
      "Epoch: 88 || iter 1152 || Loss: 1.4209 ||timer: 0.3835 sec.\n",
      "Epoch: 89 || iter 1160 || Loss: 1.1336 ||timer: 0.3804 sec.\n",
      "Epoch: 89 || iter 1168 || Loss: 1.0843 ||timer: 0.3809 sec.\n",
      "Epoch: 90 || iter 1176 || Loss: 1.5339 ||Saving state, Epoch: 90\n",
      "timer: 0.4051 sec.\n",
      "Epoch: 91 || iter 1184 || Loss: 1.1831 ||timer: 0.3894 sec.\n",
      "Epoch: 91 || iter 1192 || Loss: 1.1919 ||timer: 0.3929 sec.\n",
      "Epoch: 92 || iter 1200 || Loss: 1.0656 ||timer: 0.3916 sec.\n",
      "Epoch: 92 || iter 1208 || Loss: 0.9953 ||timer: 0.3995 sec.\n",
      "Epoch: 93 || iter 1216 || Loss: 0.9096 ||timer: 0.3810 sec.\n",
      "Epoch: 94 || iter 1224 || Loss: 1.0601 ||timer: 0.3805 sec.\n",
      "Epoch: 94 || iter 1232 || Loss: 1.0380 ||timer: 0.4132 sec.\n",
      "Epoch: 95 || iter 1240 || Loss: 1.0601 ||timer: 0.3042 sec.\n",
      "Epoch: 95 || iter 1248 || Loss: 0.9947 ||Saving state, Epoch: 95\n",
      "timer: 0.3891 sec.\n",
      "Epoch: 96 || iter 1256 || Loss: 1.0765 ||timer: 0.3768 sec.\n",
      "Epoch: 97 || iter 1264 || Loss: 1.2063 ||timer: 0.3768 sec.\n",
      "Epoch: 97 || iter 1272 || Loss: 1.0316 ||timer: 0.3817 sec.\n",
      "Epoch: 98 || iter 1280 || Loss: 0.8880 ||timer: 0.3949 sec.\n",
      "Epoch: 99 || iter 1288 || Loss: 0.8039 ||timer: 0.3991 sec.\n",
      "Epoch: 99 || iter 1296 || Loss: 0.9394 ||"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        iteration += 1\n",
    "        images, targets = batch\n",
    "        if CUDA:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "\n",
    "        # Forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.item()\n",
    "        conf_loss += loss_c.item()\n",
    "\n",
    "        if iteration % 8 == 0:\n",
    "                print('timer: %.4f sec.' % (t1 - t0))\n",
    "                print('Epoch: ' + str(epoch) + ' || iter ' + repr(PRETRAINED_EPOCH + iteration) + ' || Loss: %.4f ||' % (loss.item()), end='')\n",
    "\n",
    "    if epoch != 0 and epoch % SAVE_MODEL_EPOCH == 0:\n",
    "        print('Saving state, Epoch:', epoch)\n",
    "        torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\n",
    "                    repr(PRETRAINED_EPOCH + epoch) + '.pth')\n",
    "    if epoch in adjust_lr_epoch:\n",
    "        adjust_learning_rate(optimizer, GAMMA, epoch)\n",
    "    # Save final model\n",
    "torch.save(ssd_net.state_dict(),\n",
    "        SAVE_FOLDER + DATASET_NAME + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'step_index = 0\\nfor iteration in range(START_ITER, cfg[\\'max_iter\\']):\\n    if iteration in cfg[\\'lr_steps\\']:\\n        step_index += 1\\n        adjust_learning_rate(optimizer, GAMMA, step_index)\\n    \\n    # make sure data iter not out of range\\n    try:\\n        images, targets = next(batch_iterator)\\n        #print(targets[0][0][4].item(), label[int(targets[0][0][4].item())])\\n    except StopIteration:\\n        batch_iterator = iter(data_loader)\\n        images, targets = next(batch_iterator)\\n    if CUDA:\\n        images = Variable(images.cuda())\\n        targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\\n    else:\\n        images = Variable(images)\\n        targets = [Variable(ann, volatile=True) for ann in targets]\\n    \\n    # Forward\\n    t0 = time.time()\\n    out = net(images)\\n    # backprop\\n    optimizer.zero_grad()\\n    loss_l, loss_c = criterion(out, targets)\\n    loss = loss_l + loss_c\\n    loss.backward()\\n    optimizer.step()\\n    t1 = time.time()\\n    loc_loss += loss_l.item()\\n    conf_loss += loss_c.item()\\n    \\n    if iteration % 10 == 0:\\n            print(\\'timer: %.4f sec.\\' % (t1 - t0))\\n            print(\\'iter \\' + repr(PRETRAINED_ITER + iteration) + \\' || Loss: %.4f ||\\' % (loss.item()), end=\\'\\')\\n    \\n    if iteration != 0 and iteration % SAVE_MODEL_ITER == 0:\\n            print(\\'Saving state, iter:\\', iteration)\\n            torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\\n                       repr(PRETRAINED_ITER + iteration) + \\'.pth\\')\\n# Save final model\\ntorch.save(ssd_net.state_dict(),\\n            SAVE_FOLDER + DATASET_NAME + \\'.pth\\')'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''step_index = 0\n",
    "for iteration in range(START_ITER, cfg['max_iter']):\n",
    "    if iteration in cfg['lr_steps']:\n",
    "        step_index += 1\n",
    "        adjust_learning_rate(optimizer, GAMMA, step_index)\n",
    "    \n",
    "    # make sure data iter not out of range\n",
    "    try:\n",
    "        images, targets = next(batch_iterator)\n",
    "        #print(targets[0][0][4].item(), label[int(targets[0][0][4].item())])\n",
    "    except StopIteration:\n",
    "        batch_iterator = iter(data_loader)\n",
    "        images, targets = next(batch_iterator)\n",
    "    if CUDA:\n",
    "        images = Variable(images.cuda())\n",
    "        targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "    else:\n",
    "        images = Variable(images)\n",
    "        targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "    \n",
    "    # Forward\n",
    "    t0 = time.time()\n",
    "    out = net(images)\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_l, loss_c = criterion(out, targets)\n",
    "    loss = loss_l + loss_c\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "    loc_loss += loss_l.item()\n",
    "    conf_loss += loss_c.item()\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(PRETRAINED_ITER + iteration) + ' || Loss: %.4f ||' % (loss.item()), end='')\n",
    "    \n",
    "    if iteration != 0 and iteration % SAVE_MODEL_ITER == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\n",
    "                       repr(PRETRAINED_ITER + iteration) + '.pth')\n",
    "# Save final model\n",
    "torch.save(ssd_net.state_dict(),\n",
    "            SAVE_FOLDER + DATASET_NAME + '.pth')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
