{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/media/arg_ws3/5E703E3A703E18EB/data/subt_artifact\"\n",
    "DATASET_NAME = \"person\"\n",
    "cfg = subt\n",
    "BASE_NET = \"./weights/vgg16_reducedfc.pth\"\n",
    "DATA_DETECTION = SUBTDetection\n",
    "BATCH_SIZE = 32\n",
    "PRETRAINED_MODEL = None\n",
    "PRETRAINED_ITER = 0\n",
    "SAVE_MODEL_ITER = 500\n",
    "START_ITER = 0\n",
    "NUM_WORKERS = 4\n",
    "CUDA = True\n",
    "LR = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "GAMMA = 0.1\n",
    "VISDOM = False\n",
    "SAVE_FOLDER = \"./weights/\" + DATASET_NAME + \"/\"\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if not CUDA:\n",
    "        print(\"WTF are u wasting your CUDA device?\")\n",
    "    else:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "# Initial model weights & bias\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "# Adjust learning rate during training\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = LR * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        print(\"Change learning rate to: \", lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index: \n",
      " {'backpack': 0, 'toolbox': 3, 'extinguisher': 1, 'radio': 2, 'valve': 4}\n",
      "['backpack', 'extinguisher', 'radio', 'toolbox', 'valve', 'None']\n"
     ]
    }
   ],
   "source": [
    "dataset = DATA_DETECTION(root=DATASET_ROOT, image_sets=['train'],transform=SSDAugmentation(cfg['min_dim'], MEANS))\n",
    "\n",
    "classes = dataset.target_transform.class_to_ind\n",
    "print(\"Class to index: \\n\", classes)\n",
    "classes = sorted(classes.items(), key=lambda kv: kv[1])\n",
    "label = []\n",
    "for i in classes:\n",
    "    label.append(i[0])\n",
    "label.append('None')\n",
    "print(label)\n",
    "true_label = ['backpack', 'extinguisher', 'toolbox', 'radio', 'valve']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
      "/home/arg_ws3/ssd.pytorch/layers/modules/l2norm.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.weight,self.gamma)\n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Delcare SSD Network\n",
    "ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "net = ssd_net\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "    ssd_net.load_weights(SAVE_FOLDER + PRETRAINED_MODEL)\n",
    "else:\n",
    "    print('Initializing weights...')\n",
    "    vgg_weights = torch.load(BASE_NET) # load vgg pretrained model\n",
    "    ssd_net.extras.apply(weights_init) # Initial SSD model weights & bias\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM,\n",
    "                weight_decay=WEIGHT_DECAY)\n",
    "criterion = MultiBoxLoss(BATCH_SIZE ,cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                False, CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Training SSD on: person\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "# loss counters\n",
    "loc_loss = 0\n",
    "conf_loss = 0\n",
    "epoch = 0\n",
    "print('Loading the dataset...')\n",
    "epoch_size = len(dataset) // BATCH_SIZE\n",
    "print('Training SSD on:', DATASET_NAME)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, BATCH_SIZE,\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                shuffle=True, collate_fn=detection_collate,\n",
    "                                pin_memory=True)\n",
    "batch_iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  app.launch_new_instance()\n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 14.2039 sec.\n",
      "iter 0 || Loss: 16.9631 ||timer: 0.3456 sec.\n",
      "iter 10 || Loss: 13.8549 ||timer: 0.3826 sec.\n",
      "iter 20 || Loss: 13.9038 ||timer: 0.3843 sec.\n",
      "iter 30 || Loss: 18.7738 ||timer: 0.3858 sec.\n",
      "iter 40 || Loss: 16.3145 ||timer: 0.3849 sec.\n",
      "iter 50 || Loss: 16.7607 ||timer: 0.3927 sec.\n",
      "iter 60 || Loss: 12.4441 ||timer: 0.3766 sec.\n",
      "iter 70 || Loss: 11.0996 ||timer: 0.3807 sec.\n",
      "iter 80 || Loss: 17.6200 ||timer: 0.3808 sec.\n",
      "iter 90 || Loss: 25.8369 ||timer: 0.3908 sec.\n",
      "iter 100 || Loss: 18.1654 ||timer: 0.3792 sec.\n",
      "iter 110 || Loss: 22.1077 ||timer: 0.3831 sec.\n",
      "iter 120 || Loss: 16.1234 ||timer: 0.3944 sec.\n",
      "iter 130 || Loss: 15.5866 ||timer: 0.3774 sec.\n",
      "iter 140 || Loss: 11.0364 ||timer: 0.3880 sec.\n",
      "iter 150 || Loss: 14.4047 ||timer: 0.3803 sec.\n",
      "iter 160 || Loss: 10.2705 ||timer: 0.3469 sec.\n",
      "iter 170 || Loss: 6.8627 ||timer: 0.3798 sec.\n",
      "iter 180 || Loss: 10.6124 ||timer: 0.3892 sec.\n",
      "iter 190 || Loss: 18.0428 ||timer: 0.3844 sec.\n",
      "iter 200 || Loss: 12.5876 ||timer: 0.3837 sec.\n",
      "iter 210 || Loss: 11.7774 ||timer: 0.3839 sec.\n",
      "iter 220 || Loss: 13.7241 ||timer: 0.3786 sec.\n",
      "iter 230 || Loss: 10.5041 ||timer: 0.3839 sec.\n",
      "iter 240 || Loss: 7.9840 ||timer: 0.3813 sec.\n",
      "iter 250 || Loss: 7.5377 ||timer: 0.3825 sec.\n",
      "iter 260 || Loss: 8.1939 ||timer: 0.3846 sec.\n",
      "iter 270 || Loss: 9.1038 ||timer: 0.3834 sec.\n",
      "iter 280 || Loss: 12.9625 ||timer: 0.3905 sec.\n",
      "iter 290 || Loss: 8.4360 ||timer: 0.3848 sec.\n",
      "iter 300 || Loss: 9.4463 ||timer: 0.3886 sec.\n",
      "iter 310 || Loss: 9.3660 ||timer: 0.3827 sec.\n",
      "iter 320 || Loss: 17.9575 ||timer: 0.3771 sec.\n",
      "iter 330 || Loss: 10.8442 ||timer: 0.3865 sec.\n",
      "iter 340 || Loss: 11.6548 ||timer: 0.3827 sec.\n",
      "iter 350 || Loss: 8.9022 ||timer: 0.3898 sec.\n",
      "iter 360 || Loss: 9.8824 ||timer: 0.3851 sec.\n",
      "iter 370 || Loss: 10.0529 ||timer: 0.3905 sec.\n",
      "iter 380 || Loss: 8.5597 ||timer: 0.3780 sec.\n",
      "iter 390 || Loss: 11.0380 ||timer: 0.3793 sec.\n",
      "iter 400 || Loss: 8.4374 ||timer: 0.3915 sec.\n",
      "iter 410 || Loss: 6.8771 ||timer: 0.3856 sec.\n",
      "iter 420 || Loss: 6.7253 ||timer: 0.3803 sec.\n",
      "iter 430 || Loss: 6.2460 ||timer: 0.3806 sec.\n",
      "iter 440 || Loss: 10.3679 ||timer: 0.3848 sec.\n",
      "iter 450 || Loss: 6.9975 ||timer: 0.4004 sec.\n",
      "iter 460 || Loss: 6.7983 ||timer: 0.3863 sec.\n",
      "iter 470 || Loss: 8.1333 ||timer: 0.3863 sec.\n",
      "iter 480 || Loss: 6.4490 ||timer: 0.3800 sec.\n",
      "iter 490 || Loss: 6.2695 ||timer: 0.3769 sec.\n",
      "iter 500 || Loss: 8.5891 ||Saving state, iter: 500\n",
      "timer: 0.3857 sec.\n",
      "iter 510 || Loss: 6.4252 ||timer: 0.3834 sec.\n",
      "iter 520 || Loss: 7.4056 ||timer: 0.3800 sec.\n",
      "iter 530 || Loss: 7.5174 ||timer: 0.3920 sec.\n",
      "iter 540 || Loss: 10.0001 ||timer: 0.3807 sec.\n",
      "iter 550 || Loss: 8.4758 ||timer: 0.3825 sec.\n",
      "iter 560 || Loss: 16.7142 ||timer: 0.3828 sec.\n",
      "iter 570 || Loss: 5.9302 ||timer: 0.3913 sec.\n",
      "iter 580 || Loss: 8.2816 ||timer: 0.3878 sec.\n",
      "iter 590 || Loss: 9.1771 ||timer: 0.3966 sec.\n",
      "iter 600 || Loss: 9.6988 ||timer: 0.3859 sec.\n",
      "iter 610 || Loss: 10.5436 ||timer: 0.3860 sec.\n",
      "iter 620 || Loss: 9.9919 ||timer: 0.3912 sec.\n",
      "iter 630 || Loss: 8.3657 ||timer: 0.4002 sec.\n",
      "iter 640 || Loss: 9.0915 ||timer: 0.3788 sec.\n",
      "iter 650 || Loss: 12.6065 ||timer: 0.3841 sec.\n",
      "iter 660 || Loss: 8.0741 ||timer: 0.3816 sec.\n",
      "iter 670 || Loss: 5.4812 ||timer: 0.3801 sec.\n",
      "iter 680 || Loss: 7.1009 ||timer: 0.3799 sec.\n",
      "iter 690 || Loss: 6.6383 ||timer: 0.3848 sec.\n",
      "iter 700 || Loss: 6.8668 ||timer: 0.3811 sec.\n",
      "iter 710 || Loss: 10.6224 ||timer: 0.3818 sec.\n",
      "iter 720 || Loss: 7.8057 ||timer: 0.3828 sec.\n",
      "iter 730 || Loss: 6.3138 ||timer: 0.3875 sec.\n",
      "iter 740 || Loss: 5.7284 ||timer: 0.3840 sec.\n",
      "iter 750 || Loss: 5.8711 ||timer: 0.3845 sec.\n",
      "iter 760 || Loss: 4.7744 ||timer: 0.3850 sec.\n",
      "iter 770 || Loss: 5.6750 ||timer: 0.3900 sec.\n",
      "iter 780 || Loss: 6.2166 ||timer: 0.3834 sec.\n",
      "iter 790 || Loss: 5.8605 ||timer: 0.3853 sec.\n",
      "iter 800 || Loss: 7.0915 ||timer: 0.3808 sec.\n",
      "iter 810 || Loss: 7.8402 ||timer: 0.3778 sec.\n",
      "iter 820 || Loss: 5.4476 ||timer: 0.3827 sec.\n",
      "iter 830 || Loss: 8.0985 ||timer: 0.3844 sec.\n",
      "iter 840 || Loss: 8.5421 ||timer: 0.3845 sec.\n",
      "iter 850 || Loss: 6.1353 ||timer: 0.3815 sec.\n",
      "iter 860 || Loss: 10.2464 ||timer: 0.3762 sec.\n",
      "iter 870 || Loss: 5.9480 ||timer: 0.3855 sec.\n",
      "iter 880 || Loss: 9.6444 ||timer: 0.3833 sec.\n",
      "iter 890 || Loss: 7.2592 ||timer: 0.4072 sec.\n",
      "iter 900 || Loss: 10.3435 ||timer: 0.3992 sec.\n",
      "iter 910 || Loss: 5.3249 ||timer: 0.4071 sec.\n",
      "iter 920 || Loss: 5.6454 ||timer: 0.3869 sec.\n",
      "iter 930 || Loss: 4.4449 ||timer: 0.3853 sec.\n",
      "iter 940 || Loss: 5.8067 ||timer: 0.3812 sec.\n",
      "iter 950 || Loss: 5.1025 ||timer: 0.3793 sec.\n",
      "iter 960 || Loss: 4.9415 ||timer: 0.3919 sec.\n",
      "iter 970 || Loss: 4.6719 ||timer: 0.3890 sec.\n",
      "iter 980 || Loss: 4.7197 ||timer: 0.3966 sec.\n",
      "iter 990 || Loss: 5.2399 ||timer: 0.3818 sec.\n",
      "iter 1000 || Loss: 4.6351 ||Saving state, iter: 1000\n",
      "timer: 0.3809 sec.\n",
      "iter 1010 || Loss: 4.9929 ||timer: 0.3851 sec.\n",
      "iter 1020 || Loss: 4.5794 ||timer: 0.3832 sec.\n",
      "iter 1030 || Loss: 4.8979 ||timer: 0.3914 sec.\n",
      "iter 1040 || Loss: 6.3412 ||timer: 0.3830 sec.\n",
      "iter 1050 || Loss: 5.4810 ||timer: 0.3904 sec.\n",
      "iter 1060 || Loss: 5.3041 ||timer: 0.4037 sec.\n",
      "iter 1070 || Loss: 5.0620 ||timer: 0.3831 sec.\n",
      "iter 1080 || Loss: 4.6659 ||timer: 0.3783 sec.\n",
      "iter 1090 || Loss: 5.4803 ||timer: 0.3815 sec.\n",
      "iter 1100 || Loss: 5.2611 ||timer: 0.3843 sec.\n",
      "iter 1110 || Loss: 6.2329 ||timer: 0.3841 sec.\n",
      "iter 1120 || Loss: 4.9784 ||timer: 0.3740 sec.\n",
      "iter 1130 || Loss: 6.4183 ||timer: 0.3809 sec.\n",
      "iter 1140 || Loss: 3.2610 ||timer: 0.3765 sec.\n",
      "iter 1150 || Loss: 4.9775 ||timer: 0.3825 sec.\n",
      "iter 1160 || Loss: 5.0716 ||timer: 0.3802 sec.\n",
      "iter 1170 || Loss: 4.2450 ||timer: 0.3806 sec.\n",
      "iter 1180 || Loss: 4.9913 ||timer: 0.3840 sec.\n",
      "iter 1190 || Loss: 4.5713 ||timer: 0.3838 sec.\n",
      "iter 1200 || Loss: 3.5150 ||timer: 0.3756 sec.\n",
      "iter 1210 || Loss: 4.4439 ||timer: 0.3812 sec.\n",
      "iter 1220 || Loss: 3.5721 ||timer: 0.3919 sec.\n",
      "iter 1230 || Loss: 4.5312 ||timer: 0.3870 sec.\n",
      "iter 1240 || Loss: 4.7726 ||timer: 0.3828 sec.\n",
      "iter 1250 || Loss: 3.3930 ||timer: 0.3896 sec.\n",
      "iter 1260 || Loss: 4.5026 ||timer: 0.3979 sec.\n",
      "iter 1270 || Loss: 3.1492 ||timer: 0.3802 sec.\n",
      "iter 1280 || Loss: 4.3411 ||timer: 0.3869 sec.\n",
      "iter 1290 || Loss: 4.0125 ||timer: 0.3809 sec.\n",
      "iter 1300 || Loss: 3.7078 ||timer: 0.3829 sec.\n",
      "iter 1310 || Loss: 4.1646 ||timer: 0.3857 sec.\n",
      "iter 1320 || Loss: 4.7802 ||timer: 0.3842 sec.\n",
      "iter 1330 || Loss: 3.3691 ||timer: 0.3857 sec.\n",
      "iter 1340 || Loss: 3.8546 ||timer: 0.3800 sec.\n",
      "iter 1350 || Loss: 3.0845 ||timer: 0.3826 sec.\n",
      "iter 1360 || Loss: 3.3178 ||timer: 0.3911 sec.\n",
      "iter 1370 || Loss: 3.5244 ||timer: 0.3833 sec.\n",
      "iter 1380 || Loss: 4.0304 ||timer: 0.3850 sec.\n",
      "iter 1390 || Loss: 4.0804 ||timer: 0.3786 sec.\n",
      "iter 1400 || Loss: 3.5218 ||timer: 0.3792 sec.\n",
      "iter 1410 || Loss: 3.8652 ||timer: 0.3863 sec.\n",
      "iter 1420 || Loss: 3.4762 ||timer: 0.3873 sec.\n",
      "iter 1430 || Loss: 3.5265 ||timer: 0.3803 sec.\n",
      "iter 1440 || Loss: 3.5965 ||timer: 0.3810 sec.\n",
      "iter 1450 || Loss: 3.5398 ||timer: 0.3828 sec.\n",
      "iter 1460 || Loss: 4.3364 ||timer: 0.3851 sec.\n",
      "iter 1470 || Loss: 3.8965 ||timer: 0.3811 sec.\n",
      "iter 1480 || Loss: 3.3495 ||timer: 0.3877 sec.\n",
      "iter 1490 || Loss: 3.6085 ||timer: 0.3854 sec.\n",
      "iter 1500 || Loss: 3.5780 ||Saving state, iter: 1500\n",
      "timer: 0.3816 sec.\n",
      "iter 1510 || Loss: 3.2545 ||timer: 0.3836 sec.\n",
      "iter 1520 || Loss: 3.6271 ||timer: 0.3851 sec.\n",
      "iter 1530 || Loss: 2.9248 ||timer: 0.3847 sec.\n",
      "iter 1540 || Loss: 3.1845 ||timer: 0.3858 sec.\n",
      "iter 1550 || Loss: 3.1379 ||timer: 0.3779 sec.\n",
      "iter 1560 || Loss: 3.3529 ||timer: 0.3795 sec.\n",
      "iter 1570 || Loss: 3.6324 ||timer: 0.3774 sec.\n",
      "iter 1580 || Loss: 3.2676 ||timer: 0.3851 sec.\n",
      "iter 1590 || Loss: 3.8250 ||timer: 0.3805 sec.\n",
      "iter 1600 || Loss: 3.8098 ||timer: 0.3861 sec.\n",
      "iter 1610 || Loss: 3.2577 ||timer: 0.3890 sec.\n",
      "iter 1620 || Loss: 3.3471 ||timer: 0.3830 sec.\n",
      "iter 1630 || Loss: 3.0053 ||timer: 0.3873 sec.\n",
      "iter 1640 || Loss: 3.9302 ||timer: 0.3995 sec.\n",
      "iter 1650 || Loss: 4.2694 ||timer: 0.3816 sec.\n",
      "iter 1660 || Loss: 3.5329 ||timer: 0.3986 sec.\n",
      "iter 1670 || Loss: 2.7416 ||timer: 0.3852 sec.\n",
      "iter 1680 || Loss: 3.4973 ||timer: 0.3767 sec.\n",
      "iter 1690 || Loss: 3.3096 ||timer: 0.3867 sec.\n",
      "iter 1700 || Loss: 3.0345 ||timer: 0.3882 sec.\n",
      "iter 1710 || Loss: 3.0850 ||timer: 0.3808 sec.\n",
      "iter 1720 || Loss: 3.7186 ||timer: 0.3885 sec.\n",
      "iter 1730 || Loss: 3.6813 ||timer: 0.3855 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1740 || Loss: 3.1312 ||timer: 0.3841 sec.\n",
      "iter 1750 || Loss: 2.8793 ||timer: 0.3788 sec.\n",
      "iter 1760 || Loss: 3.0130 ||timer: 0.4413 sec.\n",
      "iter 1770 || Loss: 3.4527 ||timer: 0.4074 sec.\n",
      "iter 1780 || Loss: 3.5615 ||timer: 0.3965 sec.\n",
      "iter 1790 || Loss: 3.7266 ||timer: 0.3864 sec.\n",
      "iter 1800 || Loss: 3.0230 ||timer: 0.3903 sec.\n",
      "iter 1810 || Loss: 2.9804 ||timer: 0.3998 sec.\n",
      "iter 1820 || Loss: 2.8286 ||timer: 0.3856 sec.\n",
      "iter 1830 || Loss: 2.8353 ||timer: 0.3801 sec.\n",
      "iter 1840 || Loss: 3.8309 ||timer: 0.3858 sec.\n",
      "iter 1850 || Loss: 3.5399 ||timer: 0.3850 sec.\n",
      "iter 1860 || Loss: 3.4252 ||timer: 0.4064 sec.\n",
      "iter 1870 || Loss: 4.1283 ||timer: 0.3828 sec.\n",
      "iter 1880 || Loss: 3.3502 ||timer: 0.3938 sec.\n",
      "iter 1890 || Loss: 4.4071 ||timer: 0.3711 sec.\n",
      "iter 1900 || Loss: 2.4552 ||timer: 0.3866 sec.\n",
      "iter 1910 || Loss: 2.9383 ||timer: 0.3846 sec.\n",
      "iter 1920 || Loss: 4.3787 ||timer: 0.3812 sec.\n",
      "iter 1930 || Loss: 3.2870 ||timer: 0.3791 sec.\n",
      "iter 1940 || Loss: 3.1594 ||timer: 0.3845 sec.\n",
      "iter 1950 || Loss: 3.2554 ||timer: 0.3823 sec.\n",
      "iter 1960 || Loss: 3.2429 ||timer: 0.3900 sec.\n",
      "iter 1970 || Loss: 2.7729 ||timer: 0.4192 sec.\n",
      "iter 1980 || Loss: 3.1828 ||timer: 0.3803 sec.\n",
      "iter 1990 || Loss: 2.7128 ||timer: 0.3837 sec.\n",
      "iter 2000 || Loss: 2.9617 ||Saving state, iter: 2000\n",
      "timer: 0.3898 sec.\n",
      "iter 2010 || Loss: 3.7048 ||timer: 0.3799 sec.\n",
      "iter 2020 || Loss: 3.2423 ||timer: 0.3887 sec.\n",
      "iter 2030 || Loss: 3.5617 ||timer: 0.3782 sec.\n",
      "iter 2040 || Loss: 3.2057 ||timer: 0.3810 sec.\n",
      "iter 2050 || Loss: 3.1753 ||timer: 0.3834 sec.\n",
      "iter 2060 || Loss: 2.9328 ||timer: 0.3924 sec.\n",
      "iter 2070 || Loss: 2.8962 ||timer: 0.3858 sec.\n",
      "iter 2080 || Loss: 3.1183 ||timer: 0.3787 sec.\n",
      "iter 2090 || Loss: 2.9898 ||timer: 0.3805 sec.\n",
      "iter 2100 || Loss: 3.2828 ||timer: 0.3878 sec.\n",
      "iter 2110 || Loss: 3.3877 ||timer: 0.3781 sec.\n",
      "iter 2120 || Loss: 3.4427 ||timer: 0.3858 sec.\n",
      "iter 2130 || Loss: 2.9734 ||timer: 0.3777 sec.\n",
      "iter 2140 || Loss: 2.8877 ||timer: 0.3801 sec.\n",
      "iter 2150 || Loss: 3.6049 ||timer: 0.3821 sec.\n",
      "iter 2160 || Loss: 2.8448 ||timer: 0.3885 sec.\n",
      "iter 2170 || Loss: 3.1770 ||timer: 0.3860 sec.\n",
      "iter 2180 || Loss: 2.6323 ||timer: 0.3857 sec.\n",
      "iter 2190 || Loss: 3.0522 ||timer: 0.3891 sec.\n",
      "iter 2200 || Loss: 3.1460 ||timer: 0.3793 sec.\n",
      "iter 2210 || Loss: 3.2341 ||timer: 0.3863 sec.\n",
      "iter 2220 || Loss: 2.9787 ||timer: 0.3822 sec.\n",
      "iter 2230 || Loss: 2.7862 ||timer: 0.3821 sec.\n",
      "iter 2240 || Loss: 2.3879 ||timer: 0.3864 sec.\n",
      "iter 2250 || Loss: 3.1662 ||timer: 0.3798 sec.\n",
      "iter 2260 || Loss: 2.8578 ||timer: 0.3828 sec.\n",
      "iter 2270 || Loss: 3.5163 ||timer: 0.3908 sec.\n",
      "iter 2280 || Loss: 2.9084 ||timer: 0.3858 sec.\n",
      "iter 2290 || Loss: 3.1768 ||timer: 0.3787 sec.\n",
      "iter 2300 || Loss: 2.8231 ||timer: 0.4011 sec.\n",
      "iter 2310 || Loss: 2.9073 ||timer: 0.3820 sec.\n",
      "iter 2320 || Loss: 2.9018 ||timer: 0.3868 sec.\n",
      "iter 2330 || Loss: 3.0837 ||timer: 0.3868 sec.\n",
      "iter 2340 || Loss: 3.0904 ||timer: 0.3797 sec.\n",
      "iter 2350 || Loss: 2.7405 ||timer: 0.3831 sec.\n",
      "iter 2360 || Loss: 2.5485 ||timer: 0.3860 sec.\n",
      "iter 2370 || Loss: 3.1631 ||timer: 0.3845 sec.\n",
      "iter 2380 || Loss: 2.6282 ||timer: 0.3842 sec.\n",
      "iter 2390 || Loss: 2.4431 ||timer: 0.3792 sec.\n",
      "iter 2400 || Loss: 2.6071 ||timer: 0.3820 sec.\n",
      "iter 2410 || Loss: 2.5611 ||timer: 0.3875 sec.\n",
      "iter 2420 || Loss: 3.3377 ||timer: 0.3934 sec.\n",
      "iter 2430 || Loss: 3.0390 ||timer: 0.3844 sec.\n",
      "iter 2440 || Loss: 2.3586 ||timer: 0.3804 sec.\n",
      "iter 2450 || Loss: 2.5602 ||timer: 0.3897 sec.\n",
      "iter 2460 || Loss: 2.4362 ||timer: 0.3769 sec.\n",
      "iter 2470 || Loss: 3.3971 ||timer: 0.3816 sec.\n",
      "iter 2480 || Loss: 2.4287 ||timer: 0.3844 sec.\n",
      "iter 2490 || Loss: 2.7593 ||timer: 0.3816 sec.\n",
      "iter 2500 || Loss: 2.7460 ||Saving state, iter: 2500\n",
      "timer: 0.3772 sec.\n",
      "iter 2510 || Loss: 2.9655 ||timer: 0.3803 sec.\n",
      "iter 2520 || Loss: 2.4609 ||timer: 0.3896 sec.\n",
      "iter 2530 || Loss: 2.9057 ||timer: 0.3814 sec.\n",
      "iter 2540 || Loss: 3.1878 ||timer: 0.3817 sec.\n",
      "iter 2550 || Loss: 2.8361 ||timer: 0.3915 sec.\n",
      "iter 2560 || Loss: 2.4936 ||timer: 0.3818 sec.\n",
      "iter 2570 || Loss: 2.3409 ||timer: 0.3837 sec.\n",
      "iter 2580 || Loss: 2.8492 ||timer: 0.3405 sec.\n",
      "iter 2590 || Loss: 3.3730 ||timer: 0.3807 sec.\n",
      "iter 2600 || Loss: 3.5826 ||timer: 0.3837 sec.\n",
      "iter 2610 || Loss: 2.7858 ||timer: 0.3930 sec.\n",
      "iter 2620 || Loss: 2.8692 ||timer: 0.3832 sec.\n",
      "iter 2630 || Loss: 2.6411 ||timer: 0.3867 sec.\n",
      "iter 2640 || Loss: 2.9575 ||timer: 0.3851 sec.\n",
      "iter 2650 || Loss: 2.6985 ||timer: 0.3811 sec.\n",
      "iter 2660 || Loss: 3.0856 ||timer: 0.3906 sec.\n",
      "iter 2670 || Loss: 2.3233 ||timer: 0.3816 sec.\n",
      "iter 2680 || Loss: 2.5426 ||timer: 0.3840 sec.\n",
      "iter 2690 || Loss: 3.6639 ||timer: 0.3807 sec.\n",
      "iter 2700 || Loss: 2.5815 ||timer: 0.3792 sec.\n",
      "iter 2710 || Loss: 2.7066 ||timer: 0.3819 sec.\n",
      "iter 2720 || Loss: 2.5969 ||timer: 0.3850 sec.\n",
      "iter 2730 || Loss: 2.5765 ||timer: 0.3872 sec.\n",
      "iter 2740 || Loss: 2.8219 ||timer: 0.3808 sec.\n",
      "iter 2750 || Loss: 2.6921 ||timer: 0.3820 sec.\n",
      "iter 2760 || Loss: 2.6633 ||timer: 0.3836 sec.\n",
      "iter 2770 || Loss: 2.4088 ||timer: 0.3869 sec.\n",
      "iter 2780 || Loss: 2.5509 ||timer: 0.3837 sec.\n",
      "iter 2790 || Loss: 2.5284 ||timer: 0.3795 sec.\n",
      "iter 2800 || Loss: 2.6696 ||timer: 0.3783 sec.\n",
      "iter 2810 || Loss: 2.5857 ||timer: 0.3810 sec.\n",
      "iter 2820 || Loss: 2.3653 ||timer: 0.3943 sec.\n",
      "iter 2830 || Loss: 2.4296 ||timer: 0.3865 sec.\n",
      "iter 2840 || Loss: 2.1492 ||timer: 0.3841 sec.\n",
      "iter 2850 || Loss: 2.3422 ||timer: 0.3341 sec.\n",
      "iter 2860 || Loss: 2.5077 ||timer: 0.3432 sec.\n",
      "iter 2870 || Loss: 3.0246 ||timer: 0.3812 sec.\n",
      "iter 2880 || Loss: 2.6526 ||timer: 0.3386 sec.\n",
      "iter 2890 || Loss: 2.5243 ||timer: 0.3759 sec.\n",
      "iter 2900 || Loss: 2.4097 ||timer: 0.3428 sec.\n",
      "iter 2910 || Loss: 3.0729 ||timer: 0.3403 sec.\n",
      "iter 2920 || Loss: 2.6390 ||timer: 0.3708 sec.\n",
      "iter 2930 || Loss: 2.3063 ||timer: 0.3850 sec.\n",
      "iter 2940 || Loss: 2.6584 ||timer: 0.3838 sec.\n",
      "iter 2950 || Loss: 2.5350 ||timer: 0.3845 sec.\n",
      "iter 2960 || Loss: 2.7462 ||timer: 0.3894 sec.\n",
      "iter 2970 || Loss: 2.4774 ||timer: 0.3812 sec.\n",
      "iter 2980 || Loss: 2.8288 ||timer: 0.3864 sec.\n",
      "iter 2990 || Loss: 2.0527 ||timer: 0.3833 sec.\n",
      "iter 3000 || Loss: 2.4434 ||Saving state, iter: 3000\n",
      "timer: 0.3820 sec.\n",
      "iter 3010 || Loss: 2.3604 ||timer: 0.3833 sec.\n",
      "iter 3020 || Loss: 2.6371 ||timer: 0.3898 sec.\n",
      "iter 3030 || Loss: 2.6205 ||timer: 0.3832 sec.\n",
      "iter 3040 || Loss: 2.6388 ||timer: 0.3857 sec.\n",
      "iter 3050 || Loss: 2.7041 ||timer: 0.3916 sec.\n",
      "iter 3060 || Loss: 2.2066 ||timer: 0.3876 sec.\n",
      "iter 3070 || Loss: 2.7272 ||timer: 0.3828 sec.\n",
      "iter 3080 || Loss: 2.5067 ||timer: 0.3832 sec.\n",
      "iter 3090 || Loss: 3.4153 ||timer: 0.3917 sec.\n",
      "iter 3100 || Loss: 2.3661 ||timer: 0.3852 sec.\n",
      "iter 3110 || Loss: 2.3159 ||timer: 0.3825 sec.\n",
      "iter 3120 || Loss: 2.2416 ||timer: 0.3835 sec.\n",
      "iter 3130 || Loss: 2.8527 ||timer: 0.3858 sec.\n",
      "iter 3140 || Loss: 2.3625 ||timer: 0.3805 sec.\n",
      "iter 3150 || Loss: 2.3264 ||timer: 0.3852 sec.\n",
      "iter 3160 || Loss: 2.7541 ||timer: 0.3838 sec.\n",
      "iter 3170 || Loss: 2.0899 ||timer: 0.3846 sec.\n",
      "iter 3180 || Loss: 2.5578 ||timer: 0.3792 sec.\n",
      "iter 3190 || Loss: 2.1696 ||timer: 0.3804 sec.\n",
      "iter 3200 || Loss: 2.5594 ||timer: 0.3806 sec.\n",
      "iter 3210 || Loss: 2.2037 ||timer: 0.3829 sec.\n",
      "iter 3220 || Loss: 2.5715 ||timer: 0.3985 sec.\n",
      "iter 3230 || Loss: 2.5789 ||timer: 0.3824 sec.\n",
      "iter 3240 || Loss: 2.2932 ||timer: 0.3845 sec.\n",
      "iter 3250 || Loss: 2.4405 ||timer: 0.3855 sec.\n",
      "iter 3260 || Loss: 2.0681 ||timer: 0.3815 sec.\n",
      "iter 3270 || Loss: 2.4246 ||timer: 0.3814 sec.\n",
      "iter 3280 || Loss: 2.4734 ||timer: 0.3855 sec.\n",
      "iter 3290 || Loss: 2.1155 ||timer: 0.3797 sec.\n",
      "iter 3300 || Loss: 2.3186 ||timer: 0.3842 sec.\n",
      "iter 3310 || Loss: 2.5620 ||timer: 0.3844 sec.\n",
      "iter 3320 || Loss: 2.4468 ||timer: 0.3851 sec.\n",
      "iter 3330 || Loss: 2.1602 ||timer: 0.3904 sec.\n",
      "iter 3340 || Loss: 2.0341 ||timer: 0.3795 sec.\n",
      "iter 3350 || Loss: 2.6957 ||timer: 0.3946 sec.\n",
      "iter 3360 || Loss: 2.2707 ||timer: 0.3796 sec.\n",
      "iter 3370 || Loss: 2.5793 ||timer: 0.3839 sec.\n",
      "iter 3380 || Loss: 2.7814 ||timer: 0.3780 sec.\n",
      "iter 3390 || Loss: 2.3375 ||timer: 0.3880 sec.\n",
      "iter 3400 || Loss: 2.7467 ||timer: 0.3843 sec.\n",
      "iter 3410 || Loss: 2.1769 ||timer: 0.3849 sec.\n",
      "iter 3420 || Loss: 2.4875 ||timer: 0.3799 sec.\n",
      "iter 3430 || Loss: 3.7484 ||timer: 0.3842 sec.\n",
      "iter 3440 || Loss: 2.4071 ||timer: 0.3848 sec.\n",
      "iter 3450 || Loss: 2.0921 ||timer: 0.3815 sec.\n",
      "iter 3460 || Loss: 2.4912 ||timer: 0.3872 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3470 || Loss: 2.6593 ||timer: 0.3875 sec.\n",
      "iter 3480 || Loss: 2.3717 ||timer: 0.3895 sec.\n",
      "iter 3490 || Loss: 1.9139 ||timer: 0.3835 sec.\n",
      "iter 3500 || Loss: 2.2740 ||Saving state, iter: 3500\n",
      "timer: 0.3840 sec.\n",
      "iter 3510 || Loss: 1.9930 ||timer: 0.3841 sec.\n",
      "iter 3520 || Loss: 2.4525 ||timer: 0.3847 sec.\n",
      "iter 3530 || Loss: 2.0850 ||timer: 0.3847 sec.\n",
      "iter 3540 || Loss: 2.3230 ||timer: 0.3855 sec.\n",
      "iter 3550 || Loss: 2.1369 ||timer: 0.3866 sec.\n",
      "iter 3560 || Loss: 3.0490 ||timer: 0.3796 sec.\n",
      "iter 3570 || Loss: 2.7265 ||timer: 0.3859 sec.\n",
      "iter 3580 || Loss: 2.3639 ||timer: 0.3853 sec.\n",
      "iter 3590 || Loss: 2.7483 ||timer: 0.3837 sec.\n",
      "iter 3600 || Loss: 2.2525 ||timer: 0.3798 sec.\n",
      "iter 3610 || Loss: 2.2670 ||timer: 0.3842 sec.\n",
      "iter 3620 || Loss: 2.4710 ||timer: 0.3845 sec.\n",
      "iter 3630 || Loss: 2.4113 ||timer: 0.3797 sec.\n",
      "iter 3640 || Loss: 2.1086 ||timer: 0.3803 sec.\n",
      "iter 3650 || Loss: 2.1652 ||timer: 0.3845 sec.\n",
      "iter 3660 || Loss: 1.9295 ||timer: 0.3834 sec.\n",
      "iter 3670 || Loss: 2.4068 ||timer: 0.3507 sec.\n",
      "iter 3680 || Loss: 2.2508 ||timer: 0.3871 sec.\n",
      "iter 3690 || Loss: 1.9505 ||timer: 0.3818 sec.\n",
      "iter 3700 || Loss: 2.3698 ||timer: 0.3807 sec.\n",
      "iter 3710 || Loss: 2.3150 ||timer: 0.3807 sec.\n",
      "iter 3720 || Loss: 2.5753 ||timer: 0.3842 sec.\n",
      "iter 3730 || Loss: 2.4295 ||timer: 0.3813 sec.\n",
      "iter 3740 || Loss: 1.9908 ||timer: 0.3832 sec.\n",
      "iter 3750 || Loss: 1.9969 ||timer: 0.3862 sec.\n",
      "iter 3760 || Loss: 2.3522 ||timer: 0.3816 sec.\n",
      "iter 3770 || Loss: 2.1569 ||timer: 0.3843 sec.\n",
      "iter 3780 || Loss: 2.6180 ||timer: 0.3829 sec.\n",
      "iter 3790 || Loss: 2.2597 ||timer: 0.3833 sec.\n",
      "iter 3800 || Loss: 2.1556 ||timer: 0.3820 sec.\n",
      "iter 3810 || Loss: 2.0871 ||timer: 0.3929 sec.\n",
      "iter 3820 || Loss: 2.2924 ||timer: 0.3838 sec.\n",
      "iter 3830 || Loss: 2.0660 ||timer: 0.3808 sec.\n",
      "iter 3840 || Loss: 2.2296 ||timer: 0.3874 sec.\n",
      "iter 3850 || Loss: 2.2422 ||timer: 0.3897 sec.\n",
      "iter 3860 || Loss: 1.8680 ||timer: 0.3925 sec.\n",
      "iter 3870 || Loss: 3.2483 ||timer: 0.3867 sec.\n",
      "iter 3880 || Loss: 1.6614 ||timer: 0.3897 sec.\n",
      "iter 3890 || Loss: 2.0996 ||timer: 0.3893 sec.\n",
      "iter 3900 || Loss: 2.4907 ||timer: 0.3809 sec.\n",
      "iter 3910 || Loss: 2.1256 ||timer: 0.3773 sec.\n",
      "iter 3920 || Loss: 2.2025 ||timer: 0.3848 sec.\n",
      "iter 3930 || Loss: 2.3501 ||timer: 0.3898 sec.\n",
      "iter 3940 || Loss: 2.4026 ||timer: 0.3365 sec.\n",
      "iter 3950 || Loss: 2.5226 ||timer: 0.3938 sec.\n",
      "iter 3960 || Loss: 2.5995 ||timer: 0.3821 sec.\n",
      "iter 3970 || Loss: 2.4878 ||timer: 0.3826 sec.\n",
      "iter 3980 || Loss: 1.8050 ||timer: 0.3842 sec.\n",
      "iter 3990 || Loss: 1.9512 ||Change learning rate to:  0.0001\n",
      "timer: 0.3874 sec.\n",
      "iter 4000 || Loss: 2.4980 ||Saving state, iter: 4000\n",
      "timer: 0.3986 sec.\n",
      "iter 4010 || Loss: 1.7208 ||timer: 0.3946 sec.\n",
      "iter 4020 || Loss: 1.9841 ||timer: 0.3382 sec.\n",
      "iter 4030 || Loss: 1.9651 ||timer: 0.3349 sec.\n",
      "iter 4040 || Loss: 2.2652 ||timer: 0.3864 sec.\n",
      "iter 4050 || Loss: 1.5908 ||timer: 0.3860 sec.\n",
      "iter 4060 || Loss: 1.6967 ||timer: 0.3819 sec.\n",
      "iter 4070 || Loss: 1.9881 ||timer: 0.3424 sec.\n",
      "iter 4080 || Loss: 2.3907 ||timer: 0.3824 sec.\n",
      "iter 4090 || Loss: 1.9467 ||timer: 0.3792 sec.\n",
      "iter 4100 || Loss: 1.7355 ||timer: 0.3854 sec.\n",
      "iter 4110 || Loss: 1.7768 ||timer: 0.3818 sec.\n",
      "iter 4120 || Loss: 2.3463 ||timer: 0.3874 sec.\n",
      "iter 4130 || Loss: 1.7415 ||timer: 0.3780 sec.\n",
      "iter 4140 || Loss: 1.6222 ||timer: 0.3826 sec.\n",
      "iter 4150 || Loss: 2.5057 ||timer: 0.3921 sec.\n",
      "iter 4160 || Loss: 1.6981 ||timer: 0.3824 sec.\n",
      "iter 4170 || Loss: 1.7218 ||timer: 0.3833 sec.\n",
      "iter 4180 || Loss: 1.8859 ||timer: 0.3821 sec.\n",
      "iter 4190 || Loss: 2.3478 ||timer: 0.3808 sec.\n",
      "iter 4200 || Loss: 2.1732 ||timer: 0.3789 sec.\n",
      "iter 4210 || Loss: 1.7886 ||timer: 0.3813 sec.\n",
      "iter 4220 || Loss: 2.1800 ||timer: 0.3844 sec.\n",
      "iter 4230 || Loss: 1.9568 ||timer: 0.3830 sec.\n",
      "iter 4240 || Loss: 2.0845 ||timer: 0.3844 sec.\n",
      "iter 4250 || Loss: 1.8681 ||timer: 0.3894 sec.\n",
      "iter 4260 || Loss: 1.7954 ||timer: 0.3839 sec.\n",
      "iter 4270 || Loss: 1.9173 ||timer: 0.3872 sec.\n",
      "iter 4280 || Loss: 1.4982 ||timer: 0.4007 sec.\n",
      "iter 4290 || Loss: 1.8691 ||timer: 0.3829 sec.\n",
      "iter 4300 || Loss: 1.9193 ||timer: 0.3789 sec.\n",
      "iter 4310 || Loss: 1.9320 ||timer: 0.3855 sec.\n",
      "iter 4320 || Loss: 1.6067 ||timer: 0.3849 sec.\n",
      "iter 4330 || Loss: 1.8760 ||timer: 0.3831 sec.\n",
      "iter 4340 || Loss: 1.8695 ||timer: 0.3862 sec.\n",
      "iter 4350 || Loss: 1.8884 ||timer: 0.3954 sec.\n",
      "iter 4360 || Loss: 2.5755 ||timer: 0.3875 sec.\n",
      "iter 4370 || Loss: 2.1823 ||timer: 0.3783 sec.\n",
      "iter 4380 || Loss: 1.4878 ||timer: 0.3807 sec.\n",
      "iter 4390 || Loss: 1.8842 ||timer: 0.3829 sec.\n",
      "iter 4400 || Loss: 1.5252 ||timer: 0.3911 sec.\n",
      "iter 4410 || Loss: 2.1195 ||timer: 0.3849 sec.\n",
      "iter 4420 || Loss: 1.8301 ||timer: 0.3810 sec.\n",
      "iter 4430 || Loss: 1.6664 ||timer: 0.3914 sec.\n",
      "iter 4440 || Loss: 1.8822 ||timer: 0.3771 sec.\n",
      "iter 4450 || Loss: 2.0686 ||timer: 0.4436 sec.\n",
      "iter 4460 || Loss: 2.1382 ||timer: 0.3795 sec.\n",
      "iter 4470 || Loss: 1.8592 ||timer: 0.3827 sec.\n",
      "iter 4480 || Loss: 1.6247 ||timer: 0.3817 sec.\n",
      "iter 4490 || Loss: 1.9053 ||timer: 0.3788 sec.\n",
      "iter 4500 || Loss: 1.7095 ||Saving state, iter: 4500\n",
      "timer: 0.3905 sec.\n",
      "iter 4510 || Loss: 2.8711 ||timer: 0.3902 sec.\n",
      "iter 4520 || Loss: 1.4548 ||timer: 0.3877 sec.\n",
      "iter 4530 || Loss: 2.1523 ||timer: 0.3823 sec.\n",
      "iter 4540 || Loss: 2.1895 ||timer: 0.3891 sec.\n",
      "iter 4550 || Loss: 1.5628 ||timer: 0.3887 sec.\n",
      "iter 4560 || Loss: 1.8790 ||timer: 0.3861 sec.\n",
      "iter 4570 || Loss: 2.4510 ||timer: 0.3834 sec.\n",
      "iter 4580 || Loss: 1.5511 ||timer: 0.3812 sec.\n",
      "iter 4590 || Loss: 1.8321 ||timer: 0.3846 sec.\n",
      "iter 4600 || Loss: 2.0893 ||timer: 0.3843 sec.\n",
      "iter 4610 || Loss: 1.9950 ||timer: 0.3901 sec.\n",
      "iter 4620 || Loss: 2.0359 ||timer: 0.3878 sec.\n",
      "iter 4630 || Loss: 2.0553 ||timer: 0.3842 sec.\n",
      "iter 4640 || Loss: 2.0227 ||timer: 0.3804 sec.\n",
      "iter 4650 || Loss: 1.6506 ||timer: 0.3856 sec.\n",
      "iter 4660 || Loss: 1.8553 ||timer: 0.3823 sec.\n",
      "iter 4670 || Loss: 2.0123 ||timer: 0.3908 sec.\n",
      "iter 4680 || Loss: 1.4009 ||timer: 0.3875 sec.\n",
      "iter 4690 || Loss: 2.2617 ||timer: 0.3841 sec.\n",
      "iter 4700 || Loss: 1.6319 ||timer: 0.3807 sec.\n",
      "iter 4710 || Loss: 2.0075 ||timer: 0.3784 sec.\n",
      "iter 4720 || Loss: 1.7009 ||timer: 0.3782 sec.\n",
      "iter 4730 || Loss: 2.0759 ||timer: 0.3889 sec.\n",
      "iter 4740 || Loss: 2.4636 ||timer: 0.3812 sec.\n",
      "iter 4750 || Loss: 2.0170 ||timer: 0.3822 sec.\n",
      "iter 4760 || Loss: 1.5715 ||timer: 0.3805 sec.\n",
      "iter 4770 || Loss: 2.1598 ||timer: 0.3806 sec.\n",
      "iter 4780 || Loss: 1.8363 ||timer: 0.3946 sec.\n",
      "iter 4790 || Loss: 2.0409 ||timer: 0.3808 sec.\n",
      "iter 4800 || Loss: 1.9870 ||timer: 0.3802 sec.\n",
      "iter 4810 || Loss: 1.6427 ||timer: 0.3817 sec.\n",
      "iter 4820 || Loss: 1.9628 ||timer: 0.3837 sec.\n",
      "iter 4830 || Loss: 1.9755 ||timer: 0.3794 sec.\n",
      "iter 4840 || Loss: 1.8353 ||timer: 0.3785 sec.\n",
      "iter 4850 || Loss: 1.6857 ||timer: 0.3941 sec.\n",
      "iter 4860 || Loss: 1.8829 ||timer: 0.3834 sec.\n",
      "iter 4870 || Loss: 1.6956 ||timer: 0.3775 sec.\n",
      "iter 4880 || Loss: 1.5067 ||timer: 0.3819 sec.\n",
      "iter 4890 || Loss: 1.7743 ||timer: 0.3793 sec.\n",
      "iter 4900 || Loss: 2.0652 ||timer: 0.3787 sec.\n",
      "iter 4910 || Loss: 1.8570 ||timer: 0.3855 sec.\n",
      "iter 4920 || Loss: 1.6219 ||timer: 0.3855 sec.\n",
      "iter 4930 || Loss: 1.7102 ||timer: 0.3838 sec.\n",
      "iter 4940 || Loss: 1.6542 ||timer: 0.3869 sec.\n",
      "iter 4950 || Loss: 1.6154 ||timer: 0.3876 sec.\n",
      "iter 4960 || Loss: 2.9288 ||timer: 0.3812 sec.\n",
      "iter 4970 || Loss: 1.6849 ||timer: 0.3811 sec.\n",
      "iter 4980 || Loss: 2.0295 ||timer: 0.3830 sec.\n",
      "iter 4990 || Loss: 1.9781 ||timer: 0.3885 sec.\n",
      "iter 5000 || Loss: 1.4346 ||Saving state, iter: 5000\n",
      "timer: 0.3861 sec.\n",
      "iter 5010 || Loss: 1.7496 ||timer: 0.3866 sec.\n",
      "iter 5020 || Loss: 1.6204 ||timer: 0.3876 sec.\n",
      "iter 5030 || Loss: 1.7287 ||timer: 0.3844 sec.\n",
      "iter 5040 || Loss: 2.0913 ||timer: 0.3844 sec.\n",
      "iter 5050 || Loss: 1.8098 ||timer: 0.3881 sec.\n",
      "iter 5060 || Loss: 1.5274 ||timer: 0.3817 sec.\n",
      "iter 5070 || Loss: 1.5785 ||timer: 0.3944 sec.\n",
      "iter 5080 || Loss: 1.8352 ||timer: 0.3804 sec.\n",
      "iter 5090 || Loss: 2.8321 ||timer: 0.3924 sec.\n",
      "iter 5100 || Loss: 2.0011 ||timer: 0.3788 sec.\n",
      "iter 5110 || Loss: 1.8873 ||timer: 0.3779 sec.\n",
      "iter 5120 || Loss: 1.8356 ||timer: 0.3834 sec.\n",
      "iter 5130 || Loss: 1.9856 ||timer: 0.3812 sec.\n",
      "iter 5140 || Loss: 2.2814 ||timer: 0.3832 sec.\n",
      "iter 5150 || Loss: 1.8690 ||timer: 0.3815 sec.\n",
      "iter 5160 || Loss: 1.3146 ||timer: 0.3849 sec.\n",
      "iter 5170 || Loss: 1.9654 ||timer: 0.3855 sec.\n",
      "iter 5180 || Loss: 1.8341 ||timer: 0.3826 sec.\n",
      "iter 5190 || Loss: 1.5956 ||timer: 0.3778 sec.\n",
      "iter 5200 || Loss: 1.4594 ||timer: 0.3856 sec.\n",
      "iter 5210 || Loss: 1.6808 ||timer: 0.3765 sec.\n",
      "iter 5220 || Loss: 1.9698 ||timer: 0.3868 sec.\n",
      "iter 5230 || Loss: 1.7334 ||timer: 0.3788 sec.\n",
      "iter 5240 || Loss: 1.7341 ||timer: 0.3816 sec.\n",
      "iter 5250 || Loss: 1.5817 ||timer: 0.3783 sec.\n",
      "iter 5260 || Loss: 2.2206 ||timer: 0.3816 sec.\n",
      "iter 5270 || Loss: 2.1716 ||timer: 0.3838 sec.\n",
      "iter 5280 || Loss: 1.7908 ||timer: 0.3814 sec.\n",
      "iter 5290 || Loss: 1.5216 ||timer: 0.3790 sec.\n",
      "iter 5300 || Loss: 1.7298 ||timer: 0.3898 sec.\n",
      "iter 5310 || Loss: 2.0379 ||timer: 0.3834 sec.\n",
      "iter 5320 || Loss: 1.9041 ||timer: 0.3816 sec.\n",
      "iter 5330 || Loss: 1.6127 ||timer: 0.3903 sec.\n",
      "iter 5340 || Loss: 2.2916 ||timer: 0.3872 sec.\n",
      "iter 5350 || Loss: 1.6339 ||timer: 0.3925 sec.\n",
      "iter 5360 || Loss: 1.7041 ||timer: 0.3887 sec.\n",
      "iter 5370 || Loss: 1.6910 ||timer: 0.3992 sec.\n",
      "iter 5380 || Loss: 1.7860 ||timer: 0.3859 sec.\n",
      "iter 5390 || Loss: 1.6929 ||timer: 0.3844 sec.\n",
      "iter 5400 || Loss: 1.7883 ||timer: 0.3800 sec.\n",
      "iter 5410 || Loss: 1.9478 ||timer: 0.3837 sec.\n",
      "iter 5420 || Loss: 1.6381 ||timer: 0.3803 sec.\n",
      "iter 5430 || Loss: 2.0848 ||timer: 0.3810 sec.\n",
      "iter 5440 || Loss: 2.2732 ||timer: 0.3803 sec.\n",
      "iter 5450 || Loss: 2.0908 ||timer: 0.3825 sec.\n",
      "iter 5460 || Loss: 1.8954 ||timer: 0.3809 sec.\n",
      "iter 5470 || Loss: 2.2301 ||timer: 0.3830 sec.\n",
      "iter 5480 || Loss: 2.0977 ||timer: 0.3933 sec.\n",
      "iter 5490 || Loss: 1.8746 ||timer: 0.3805 sec.\n",
      "iter 5500 || Loss: 1.7996 ||Saving state, iter: 5500\n",
      "timer: 0.3803 sec.\n",
      "iter 5510 || Loss: 1.8369 ||timer: 0.3799 sec.\n",
      "iter 5520 || Loss: 1.8108 ||timer: 0.3811 sec.\n",
      "iter 5530 || Loss: 1.7443 ||timer: 0.3847 sec.\n",
      "iter 5540 || Loss: 1.5518 ||timer: 0.3840 sec.\n",
      "iter 5550 || Loss: 1.5692 ||timer: 0.3866 sec.\n",
      "iter 5560 || Loss: 1.7906 ||timer: 0.3841 sec.\n",
      "iter 5570 || Loss: 2.2956 ||timer: 0.3794 sec.\n",
      "iter 5580 || Loss: 1.6717 ||timer: 0.3860 sec.\n",
      "iter 5590 || Loss: 1.8528 ||timer: 0.3913 sec.\n",
      "iter 5600 || Loss: 1.5964 ||timer: 0.3952 sec.\n",
      "iter 5610 || Loss: 1.6795 ||timer: 0.3881 sec.\n",
      "iter 5620 || Loss: 1.7961 ||timer: 0.3797 sec.\n",
      "iter 5630 || Loss: 1.9199 ||timer: 0.3816 sec.\n",
      "iter 5640 || Loss: 1.6542 ||timer: 0.4315 sec.\n",
      "iter 5650 || Loss: 2.0829 ||timer: 0.3786 sec.\n",
      "iter 5660 || Loss: 1.8411 ||timer: 0.3849 sec.\n",
      "iter 5670 || Loss: 1.8041 ||timer: 0.3817 sec.\n",
      "iter 5680 || Loss: 2.1534 ||timer: 0.4333 sec.\n",
      "iter 5690 || Loss: 2.0548 ||timer: 0.3897 sec.\n",
      "iter 5700 || Loss: 1.4988 ||timer: 0.3989 sec.\n",
      "iter 5710 || Loss: 1.7640 ||timer: 0.3890 sec.\n",
      "iter 5720 || Loss: 1.6751 ||timer: 0.4315 sec.\n",
      "iter 5730 || Loss: 1.8874 ||timer: 0.3918 sec.\n",
      "iter 5740 || Loss: 2.1578 ||timer: 0.4068 sec.\n",
      "iter 5750 || Loss: 1.8047 ||timer: 0.3811 sec.\n",
      "iter 5760 || Loss: 1.8694 ||timer: 0.3842 sec.\n",
      "iter 5770 || Loss: 1.5001 ||timer: 0.3801 sec.\n",
      "iter 5780 || Loss: 1.5688 ||timer: 0.4329 sec.\n",
      "iter 5790 || Loss: 1.5216 ||timer: 0.3870 sec.\n",
      "iter 5800 || Loss: 1.4797 ||timer: 0.3883 sec.\n",
      "iter 5810 || Loss: 1.7035 ||timer: 0.3817 sec.\n",
      "iter 5820 || Loss: 1.7986 ||timer: 0.3878 sec.\n",
      "iter 5830 || Loss: 1.8496 ||timer: 0.3834 sec.\n",
      "iter 5840 || Loss: 2.0194 ||timer: 0.3892 sec.\n",
      "iter 5850 || Loss: 1.8530 ||timer: 0.3772 sec.\n",
      "iter 5860 || Loss: 1.8465 ||timer: 0.3811 sec.\n",
      "iter 5870 || Loss: 1.6600 ||timer: 0.3874 sec.\n",
      "iter 5880 || Loss: 1.6948 ||timer: 0.3907 sec.\n",
      "iter 5890 || Loss: 2.0149 ||timer: 0.3859 sec.\n",
      "iter 5900 || Loss: 2.1069 ||timer: 0.3834 sec.\n",
      "iter 5910 || Loss: 1.9517 ||timer: 0.3787 sec.\n",
      "iter 5920 || Loss: 1.4695 ||timer: 0.3809 sec.\n",
      "iter 5930 || Loss: 2.0077 ||timer: 0.3854 sec.\n",
      "iter 5940 || Loss: 1.9104 ||timer: 0.3860 sec.\n",
      "iter 5950 || Loss: 2.2247 ||timer: 0.3807 sec.\n",
      "iter 5960 || Loss: 2.2460 ||timer: 0.3843 sec.\n",
      "iter 5970 || Loss: 1.5456 ||timer: 0.3896 sec.\n",
      "iter 5980 || Loss: 1.9191 ||timer: 0.3961 sec.\n",
      "iter 5990 || Loss: 1.4647 ||timer: 0.3819 sec.\n",
      "iter 6000 || Loss: 1.8029 ||Saving state, iter: 6000\n",
      "timer: 0.3835 sec.\n",
      "iter 6010 || Loss: 1.7562 ||timer: 0.3826 sec.\n",
      "iter 6020 || Loss: 1.6146 ||timer: 0.3843 sec.\n",
      "iter 6030 || Loss: 1.8590 ||timer: 0.3841 sec.\n",
      "iter 6040 || Loss: 1.6322 ||timer: 0.3799 sec.\n",
      "iter 6050 || Loss: 1.5524 ||timer: 0.3881 sec.\n",
      "iter 6060 || Loss: 1.9797 ||timer: 0.3819 sec.\n",
      "iter 6070 || Loss: 1.9142 ||timer: 0.3894 sec.\n",
      "iter 6080 || Loss: 2.1910 ||timer: 0.3816 sec.\n",
      "iter 6090 || Loss: 1.6161 ||timer: 0.3772 sec.\n",
      "iter 6100 || Loss: 1.7159 ||timer: 0.3913 sec.\n",
      "iter 6110 || Loss: 1.7934 ||timer: 0.3872 sec.\n",
      "iter 6120 || Loss: 1.7246 ||timer: 0.3813 sec.\n",
      "iter 6130 || Loss: 1.7322 ||timer: 0.3780 sec.\n",
      "iter 6140 || Loss: 1.8800 ||timer: 0.3953 sec.\n",
      "iter 6150 || Loss: 1.5520 ||timer: 0.3635 sec.\n",
      "iter 6160 || Loss: 1.8572 ||timer: 0.3869 sec.\n",
      "iter 6170 || Loss: 1.6713 ||timer: 0.3839 sec.\n",
      "iter 6180 || Loss: 1.6605 ||timer: 0.3839 sec.\n",
      "iter 6190 || Loss: 1.8198 ||timer: 0.3771 sec.\n",
      "iter 6200 || Loss: 1.3824 ||timer: 0.3770 sec.\n",
      "iter 6210 || Loss: 1.6471 ||timer: 0.3824 sec.\n",
      "iter 6220 || Loss: 1.5942 ||timer: 0.3858 sec.\n",
      "iter 6230 || Loss: 1.8406 ||timer: 0.3805 sec.\n",
      "iter 6240 || Loss: 1.8028 ||timer: 0.3822 sec.\n",
      "iter 6250 || Loss: 1.9982 ||timer: 0.3767 sec.\n",
      "iter 6260 || Loss: 1.8886 ||timer: 0.4086 sec.\n",
      "iter 6270 || Loss: 1.6410 ||timer: 0.3908 sec.\n",
      "iter 6280 || Loss: 1.8897 ||timer: 0.3792 sec.\n",
      "iter 6290 || Loss: 1.7837 ||timer: 0.3784 sec.\n",
      "iter 6300 || Loss: 1.8423 ||timer: 0.3665 sec.\n",
      "iter 6310 || Loss: 1.8307 ||timer: 0.3780 sec.\n",
      "iter 6320 || Loss: 1.7650 ||timer: 0.3793 sec.\n",
      "iter 6330 || Loss: 1.7508 ||timer: 0.3817 sec.\n",
      "iter 6340 || Loss: 1.8563 ||timer: 0.3797 sec.\n",
      "iter 6350 || Loss: 1.6058 ||timer: 0.3868 sec.\n",
      "iter 6360 || Loss: 1.7582 ||timer: 0.3822 sec.\n",
      "iter 6370 || Loss: 1.9162 ||timer: 0.3807 sec.\n",
      "iter 6380 || Loss: 1.5606 ||timer: 0.3823 sec.\n",
      "iter 6390 || Loss: 1.5061 ||timer: 0.3856 sec.\n",
      "iter 6400 || Loss: 1.8117 ||timer: 0.3820 sec.\n",
      "iter 6410 || Loss: 1.4761 ||timer: 0.3907 sec.\n",
      "iter 6420 || Loss: 1.8778 ||timer: 0.3851 sec.\n",
      "iter 6430 || Loss: 1.7317 ||timer: 0.3863 sec.\n",
      "iter 6440 || Loss: 1.7445 ||timer: 0.3862 sec.\n",
      "iter 6450 || Loss: 1.4539 ||timer: 0.3817 sec.\n",
      "iter 6460 || Loss: 1.7392 ||timer: 0.3827 sec.\n",
      "iter 6470 || Loss: 2.0714 ||timer: 0.3862 sec.\n",
      "iter 6480 || Loss: 2.0117 ||timer: 0.3860 sec.\n",
      "iter 6490 || Loss: 2.1779 ||timer: 0.3833 sec.\n",
      "iter 6500 || Loss: 2.0325 ||Saving state, iter: 6500\n",
      "timer: 0.3817 sec.\n",
      "iter 6510 || Loss: 2.1037 ||timer: 0.3875 sec.\n",
      "iter 6520 || Loss: 1.8258 ||timer: 0.3844 sec.\n",
      "iter 6530 || Loss: 1.8485 ||timer: 0.3848 sec.\n",
      "iter 6540 || Loss: 1.5532 ||timer: 0.3777 sec.\n",
      "iter 6550 || Loss: 1.5854 ||timer: 0.3825 sec.\n",
      "iter 6560 || Loss: 1.9429 ||timer: 0.3361 sec.\n",
      "iter 6570 || Loss: 1.9324 ||timer: 0.3782 sec.\n",
      "iter 6580 || Loss: 2.0793 ||timer: 0.3834 sec.\n",
      "iter 6590 || Loss: 1.7057 ||timer: 0.3894 sec.\n",
      "iter 6600 || Loss: 1.6454 ||timer: 0.3843 sec.\n",
      "iter 6610 || Loss: 1.6927 ||timer: 0.3824 sec.\n",
      "iter 6620 || Loss: 1.6892 ||timer: 0.3830 sec.\n",
      "iter 6630 || Loss: 1.8998 ||timer: 0.3885 sec.\n",
      "iter 6640 || Loss: 1.9115 ||timer: 0.3799 sec.\n",
      "iter 6650 || Loss: 1.5237 ||timer: 0.3825 sec.\n",
      "iter 6660 || Loss: 1.8574 ||timer: 0.3886 sec.\n",
      "iter 6670 || Loss: 1.7215 ||timer: 0.3884 sec.\n",
      "iter 6680 || Loss: 2.3447 ||timer: 0.3852 sec.\n",
      "iter 6690 || Loss: 1.9843 ||timer: 0.3841 sec.\n",
      "iter 6700 || Loss: 1.4619 ||timer: 0.3883 sec.\n",
      "iter 6710 || Loss: 1.5637 ||timer: 0.3791 sec.\n",
      "iter 6720 || Loss: 1.6761 ||timer: 0.3863 sec.\n",
      "iter 6730 || Loss: 1.9044 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4b79af0de005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ssd.pytorch/layers/modules/multibox_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             match(self.threshold, truths, defaults, self.variance, labels,\n\u001b[0;32m---> 75\u001b[0;31m                   loc_t, conf_t, idx)\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ssd.pytorch/layers/box_utils.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(threshold, truths, priors, variances, labels, loc_t, conf_t, idx)\u001b[0m\n\u001b[1;32m     89\u001b[0m     overlaps = jaccard(\n\u001b[1;32m     90\u001b[0m         \u001b[0mtruths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mpoint_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# (Bipartite Matching)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ssd.pytorch/layers/box_utils.py\u001b[0m in \u001b[0;36mjaccard\u001b[0;34m(box_a, box_b)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mjaccard\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     area_a = ((box_a[:, 2]-box_a[:, 0]) *\n\u001b[1;32m     64\u001b[0m               (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
      "\u001b[0;32m~/ssd.pytorch/layers/box_utils.py\u001b[0m in \u001b[0;36mintersect\u001b[0;34m(box_a, box_b)\u001b[0m\n\u001b[1;32m     43\u001b[0m                        box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n\u001b[1;32m     44\u001b[0m     min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n\u001b[0;32m---> 45\u001b[0;31m                        box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_xy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_xy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step_index = 0\n",
    "for iteration in range(START_ITER, cfg['max_iter']):\n",
    "    if iteration in cfg['lr_steps']:\n",
    "        step_index += 1\n",
    "        adjust_learning_rate(optimizer, GAMMA, step_index)\n",
    "    \n",
    "    # make sure data iter not out of range\n",
    "    try:\n",
    "        images, targets = next(batch_iterator)\n",
    "        #print(targets[0][0][4].item(), label[int(targets[0][0][4].item())])\n",
    "    except StopIteration:\n",
    "        batch_iterator = iter(data_loader)\n",
    "        images, targets = next(batch_iterator)\n",
    "    if CUDA:\n",
    "        images = Variable(images.cuda())\n",
    "        targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "    else:\n",
    "        images = Variable(images)\n",
    "        targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "    \n",
    "    # Forward\n",
    "    t0 = time.time()\n",
    "    out = net(images)\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_l, loss_c = criterion(out, targets)\n",
    "    loss = loss_l + loss_c\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "    loc_loss += loss_l.item()\n",
    "    conf_loss += loss_c.item()\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(PRETRAINED_ITER + iteration) + ' || Loss: %.4f ||' % (loss.item()), end='')\n",
    "    \n",
    "    if iteration != 0 and iteration % SAVE_MODEL_ITER == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\n",
    "                       repr(PRETRAINED_ITER + iteration) + '.pth')\n",
    "# Save final model\n",
    "torch.save(ssd_net.state_dict(),\n",
    "            SAVE_FOLDER + DATASET_NAME + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
