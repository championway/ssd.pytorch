{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/media/arg_ws3/5E703E3A703E18EB/data/mm_FCN\"\n",
    "DATASET_NAME = \"unity\"\n",
    "cfg = mmfcn\n",
    "BASE_NET = \"./weights/vgg16_reducedfc.pth\"\n",
    "DATA_DETECTION = MMFCNDetection\n",
    "BATCH_SIZE = 32\n",
    "#PRETRAINED_MODEL = \"/home/arg_ws3/ssd.pytorch/weights/person/person_67500.pth\"\n",
    "PRETRAINED_MODEL = None\n",
    "PRETRAINED_EPOCH = 0\n",
    "SAVE_MODEL_EPOCH = 1\n",
    "START_ITER = 0\n",
    "NUM_WORKERS = 8\n",
    "EPOCH = 61\n",
    "adjust_lr_epoch = [15, 30, 45]\n",
    "CUDA = True\n",
    "LR = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "GAMMA = 0.1\n",
    "VISDOM = False\n",
    "SAVE_FOLDER = \"/media/arg_ws3/5E703E3A703E18EB/research/mm_fcn/ssd/\" + DATASET_NAME + \"/\"\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if not CUDA:\n",
    "        print(\"WTF are u wasting your CUDA device?\")\n",
    "    else:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "# Initial model weights & bias\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "# Adjust learning rate during training\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step \n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    #lr = LR * (gamma ** (step))\n",
    "    lr = LR * (gamma)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        print(\"Change learning rate to: \", lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index: \n",
      " {'pocky': 14, 'macadamia': 11, 'hunts': 6, 'vanish': 18, 'kotex': 9, 'heineken': 5, 'libava': 10, 'kleenex': 8, 'kellogg': 7, 'cocacola': 2, 'raisins': 15, 'stax': 16, 'crayola': 3, 'mm': 13, 'folgers': 4, 'swissmiss': 17, 'andes': 1, 'viva': 19, '3m': 0, 'milo': 12}\n",
      "['3m', 'andes', 'cocacola', 'crayola', 'folgers', 'heineken', 'hunts', 'kellogg', 'kleenex', 'kotex', 'libava', 'macadamia', 'milo', 'mm', 'pocky', 'raisins', 'stax', 'swissmiss', 'vanish', 'viva', 'None']\n"
     ]
    }
   ],
   "source": [
    "dataset = DATA_DETECTION(root=DATASET_ROOT, image_sets=['train'],transform=SSDAugmentation(cfg['min_dim'], MEANS))\n",
    "\n",
    "classes = dataset.target_transform.class_to_ind\n",
    "print(\"Class to index: \\n\", classes)\n",
    "classes = sorted(classes.items(), key=lambda kv: kv[1])\n",
    "label = []\n",
    "for i in classes:\n",
    "    label.append(i[0])\n",
    "label.append('None')\n",
    "print(label)\n",
    "true_label = ['person', 'palm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
      "/home/arg_ws3/ssd.pytorch/layers/modules/l2norm.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.weight,self.gamma)\n"
     ]
    }
   ],
   "source": [
    "ssd_pretrained = build_ssd('train', cfg['min_dim'], 3)\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_pretrained)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "    ssd_pretrained.load_weights(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Initializing weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Delcare SSD Network\n",
    "ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "print(cfg['num_classes'])\n",
    "net = ssd_net\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "SAME_CLASS = False\n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    if SAME_CLASS:\n",
    "        print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "        ssd_net.load_weights(PRETRAINED_MODEL)\n",
    "    else:\n",
    "        print('Load pretrained model with different classes')\n",
    "        ssd_pretrained = build_ssd('train', cfg['min_dim'], 2)\n",
    "        ssd_pretrained.load_weights(PRETRAINED_MODEL)\n",
    "        ssd_net.vgg = ssd_pretrained.vgg\n",
    "        ssd_net.extras = ssd_pretrained.extras\n",
    "        ssd_net.loc = ssd_pretrained.loc\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "else:\n",
    "    print('Initializing weights...')\n",
    "    vgg_weights = torch.load(BASE_NET) # load vgg pretrained model\n",
    "    ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "    ssd_net.extras.apply(weights_init) # Initial SSD model weights & bias\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM,\n",
    "                weight_decay=WEIGHT_DECAY)\n",
    "criterion = MultiBoxLoss(BATCH_SIZE ,cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                False, CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(list(ssd_net.parameters()))-30):\n",
    "#    list(ssd_net.parameters())[i].data = list(ssd_pretrained.parameters())[i]\n",
    "#list(ssd_net.conf)[-1] = list(ssd_pretrained.conf)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Training SSD on: unity\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "# loss counters\n",
    "loc_loss = 0\n",
    "conf_loss = 0\n",
    "epoch = 0\n",
    "print('Loading the dataset...')\n",
    "epoch_size = len(dataset) // BATCH_SIZE\n",
    "print('Training SSD on:', DATASET_NAME)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, BATCH_SIZE,\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                shuffle=True, collate_fn=detection_collate,\n",
    "                                pin_memory=True)\n",
    "batch_iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 0.3635 sec.\n",
      "Epoch: 0 || iter 8 || Loss: 21.4768 ||timer: 0.3926 sec.\n",
      "Epoch: 0 || iter 16 || Loss: 16.2119 ||timer: 0.3961 sec.\n",
      "Epoch: 0 || iter 24 || Loss: 15.6092 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-02bbf7526157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ssd.pytorch/layers/modules/multibox_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mneg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mconf_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mneg_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mtargets_weighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_weighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        iteration += 1\n",
    "        images, targets = batch\n",
    "        if CUDA:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "\n",
    "        # Forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.item()\n",
    "        conf_loss += loss_c.item()\n",
    "\n",
    "        if iteration % 8 == 0:\n",
    "                print('timer: %.4f sec.' % (t1 - t0))\n",
    "                print('Epoch: ' + str(epoch) + ' || iter ' + repr(PRETRAINED_EPOCH + iteration) + ' || Loss: %.4f ||' % (loss.item()), end='')\n",
    "\n",
    "    if epoch != 0 and epoch % SAVE_MODEL_EPOCH == 0:\n",
    "        print('Saving state, Epoch:', epoch)\n",
    "        torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\n",
    "                    repr(PRETRAINED_EPOCH + epoch) + '.pth')\n",
    "    if epoch in adjust_lr_epoch:\n",
    "        adjust_learning_rate(optimizer, GAMMA, epoch)\n",
    "    # Save final model\n",
    "torch.save(ssd_net.state_dict(),\n",
    "        SAVE_FOLDER + DATASET_NAME + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''step_index = 0\n",
    "for iteration in range(START_ITER, cfg['max_iter']):\n",
    "    if iteration in cfg['lr_steps']:\n",
    "        step_index += 1\n",
    "        adjust_learning_rate(optimizer, GAMMA, step_index)\n",
    "    \n",
    "    # make sure data iter not out of range\n",
    "    try:\n",
    "        images, targets = next(batch_iterator)\n",
    "        #print(targets[0][0][4].item(), label[int(targets[0][0][4].item())])\n",
    "    except StopIteration:\n",
    "        batch_iterator = iter(data_loader)\n",
    "        images, targets = next(batch_iterator)\n",
    "    if CUDA:\n",
    "        images = Variable(images.cuda())\n",
    "        targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "    else:\n",
    "        images = Variable(images)\n",
    "        targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "    \n",
    "    # Forward\n",
    "    t0 = time.time()\n",
    "    out = net(images)\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_l, loss_c = criterion(out, targets)\n",
    "    loss = loss_l + loss_c\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "    loc_loss += loss_l.item()\n",
    "    conf_loss += loss_c.item()\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(PRETRAINED_ITER + iteration) + ' || Loss: %.4f ||' % (loss.item()), end='')\n",
    "    \n",
    "    if iteration != 0 and iteration % SAVE_MODEL_ITER == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            torch.save(ssd_net.state_dict(), SAVE_FOLDER + DATASET_NAME + \"_\" +\n",
    "                       repr(PRETRAINED_ITER + iteration) + '.pth')\n",
    "# Save final model\n",
    "torch.save(ssd_net.state_dict(),\n",
    "            SAVE_FOLDER + DATASET_NAME + '.pth')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
