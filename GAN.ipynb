{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from pix2pix_models import *\n",
    "from pix2pix_datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD\n",
    "DATASET_ROOT = \"/media/arg_ws3/5E703E3A703E18EB/data/argbot\"\n",
    "DATASET_NAME = \"person_mask\"\n",
    "cfg = argbot\n",
    "BASE_NET = \"./weights/vgg16_reducedfc.pth\"\n",
    "DATA_DETECTION = ARGBOTDetection\n",
    "BATCH_SIZE = 1\n",
    "PRETRAINED_MODEL = \"/home/arg_ws3/ssd.pytorch/weights/person/person_20000.pth\"\n",
    "#PRETRAINED_MODEL = None\n",
    "PRETRAINED_EPOCH = 0\n",
    "SAVE_MODEL_EPOCH = 1\n",
    "START_ITER = 0\n",
    "NUM_WORKERS = 8\n",
    "EPOCH = 300\n",
    "adjust_lr_epoch = [60, 80, 150]\n",
    "CUDA = True\n",
    "LR = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "GAMMA = 0.1\n",
    "VISDOM = False\n",
    "\n",
    "# GAN\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "lr = 0.002\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "sample_interval = 200\n",
    "channels = 3\n",
    "Tensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if not CUDA:\n",
    "        print(\"WTF are u wasting your CUDA device?\")\n",
    "    else:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "# Initial model weights & bias\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "# Adjust learning rate during training\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    #lr = LR * (gamma ** (step))\n",
    "    lr = LR * (gamma)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        print(\"Change learning rate to: \", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training, loading /home/arg_ws3/ssd.pytorch/weights/person/person_20000.pth...\n",
      "Loading weights into state dict...\n",
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/ssd.pytorch/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
      "/home/arg_ws3/ssd.pytorch/layers/modules/l2norm.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.weight,self.gamma)\n"
     ]
    }
   ],
   "source": [
    "ssd_pretrained = build_ssd('train', cfg['min_dim'], 2)\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_pretrained)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "    ssd_pretrained.load_weights(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training, loading /home/arg_ws3/ssd.pytorch/weights/person/person_20000.pth...\n",
      "Loading weights into state dict...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Delcare SSD Network\n",
    "ssd_net = build_ssd('train', cfg['min_dim'], 2)\n",
    "net = ssd_net\n",
    "if CUDA:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "SAME_CLASS = True\n",
    "if PRETRAINED_MODEL is not None: # Use SSD pretrained model\n",
    "    if SAME_CLASS:\n",
    "        print('Resuming training, loading {}...'.format(PRETRAINED_MODEL))\n",
    "        ssd_net.load_weights(PRETRAINED_MODEL)\n",
    "    else:\n",
    "        print('Load pretrained model with different classes')\n",
    "        ssd_pretrained = build_ssd('train', cfg['min_dim'], 2)\n",
    "        ssd_pretrained.load_weights(PRETRAINED_MODEL)\n",
    "        ssd_net.vgg = ssd_pretrained.vgg\n",
    "        ssd_net.extras = ssd_pretrained.extras\n",
    "        ssd_net.loc = ssd_pretrained.loc\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "else:\n",
    "    print('Initializing weights...')\n",
    "    vgg_weights = torch.load(BASE_NET) # load vgg pretrained model\n",
    "    ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "    ssd_net.extras.apply(weights_init) # Initial SSD model weights & bias\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)\n",
    "\n",
    "optimizer_SSD = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM,\n",
    "                weight_decay=WEIGHT_DECAY)\n",
    "criterion = MultiBoxLoss(BATCH_SIZE ,2, 0.5, True, 0, True, 3, 0.5,\n",
    "                False, CUDA)\n",
    "net.train()\n",
    "loc_loss = 0\n",
    "conf_loss = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"gan_test_pre\"\n",
    "os.makedirs('images/%s' % dataset_name, exist_ok=True)\n",
    "os.makedirs('saved_models/%s' % dataset_name, exist_ok=True)\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "criterion_bbx = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_height//2**4, img_width//2**4)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorUNet(in_channels=3, out_channels=3)\n",
    "discriminator = Discriminator(in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_pixelwise.cuda()\n",
    "    criterion_bbx.cuda()\n",
    "epoch = 0\n",
    "if True:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load('/home/arg_ws3/ssd.pytorch/saved_models/gan_test/generator_14.pth'))\n",
    "    #discriminator.load_state_dict(torch.load('saved_models/%s/discriminator_%d.pth' % (dataset_name, epoch)))\n",
    "    discriminator.apply(weights_init_normal)\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Configure dataloaders\n",
    "transforms_A = [ transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "transforms_B = [ transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "                transforms.ToTensor() ]\n",
    "\n",
    "inv_normalize = transforms.Compose([ \n",
    "                transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                             std = [ 1/0.5, 1/0.5, 1/0.5 ]),\n",
    "                transforms.Normalize(mean = [ -0.5, -0.5, -0.5 ],\n",
    "                                             std = [ 1., 1., 1. ]),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dataloader = DataLoader(ImageDataset(transform_=transforms_A),\\n                        batch_size=1, shuffle=True, num_workers=n_cpu)\\n\\nval_dataloader = DataLoader(ImageDataset(transform_=transforms_A, mode='test'),\\n                            batch_size=1, shuffle=True, num_workers=1)\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dataloader = DataLoader(ImageDataset(transform_=transforms_A),\n",
    "                        batch_size=1, shuffle=True, num_workers=n_cpu)\n",
    "\n",
    "val_dataloader = DataLoader(ImageDataset(transform_=transforms_A, mode='test'),\n",
    "                            batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.7024, 0.4157, 0.8204, 0.8196, 0.0000]], device='cpu')]\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "dataset = DATA_DETECTION(root=DATASET_ROOT, image_sets=['person_train'],transform=SSDAugmentation(cfg['min_dim'], MEANS))\n",
    "data_loader = data.DataLoader(dataset, 1,\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                shuffle=True, collate_fn=detection_collate,\n",
    "                                pin_memory=True)\n",
    "batch_iterator = iter(data_loader)\n",
    "a = next(batch_iterator)\n",
    "print(a[1])\n",
    "new_img=F.upsample(a[0], scale_factor=1/1.171875, mode='bilinear')\n",
    "\n",
    "print(new_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"batch_iterator = iter(dataloader)\\na = next(batch_iterator)\\nprint(a[1].shape)\\na[0].shape\\nnew_img=F.upsample(a[0], scale_factor=1.171875, mode='bilinear')\\nnew_img = torch.cat((new_img, new_img, new_img), dim=1)\\nnew_img.shape\\nprint(a[1])\\ntargets = [Variable(ann.cuda(), volatile=True) for ann in a[1]]\\ntargets\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''batch_iterator = iter(dataloader)\n",
    "a = next(batch_iterator)\n",
    "print(a[1].shape)\n",
    "a[0].shape\n",
    "new_img=F.upsample(a[0], scale_factor=1.171875, mode='bilinear')\n",
    "new_img = torch.cat((new_img, new_img, new_img), dim=1)\n",
    "new_img.shape\n",
    "print(a[1])\n",
    "targets = [Variable(ann.cuda(), volatile=True) for ann in a[1]]\n",
    "targets'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs, bbx = next(iter(data_loader))\n",
    "    real = Variable(imgs.type(Tensor))\n",
    "    real = F.upsample(real, scale_factor=1/1.171875, mode='bilinear')\n",
    "    fake = generator(real)\n",
    "    img_sample = torch.cat((real.data, fake.data), -1)\n",
    "    save_image(img_sample, 'images/%s/%s.png' % (dataset_name, batches_done), nrow=5, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 7291/7292] [G loss: 3.018299] [loss: 3.018299] ETA: 4 days, 6:58:58.467663 Saving state, Epoch: 0\n",
      "[Epoch 1/300] [Batch 7291/7292] [G loss: 2.761356] [loss: 2.761356] ETA: 4 days, 6:34:01.050696 Saving state, Epoch: 1\n",
      "[Epoch 2/300] [Batch 7291/7292] [G loss: 3.828712] [loss: 3.828712] ETA: 4 days, 7:31:51.090922 Saving state, Epoch: 2\n",
      "[Epoch 3/300] [Batch 2486/7292] [G loss: 4.284740] [loss: 4.284740] ETA: 4 days, 7:09:56.505126 7  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c52c7e582d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                                         (epoch, EPOCH,\n\u001b[1;32m     91\u001b[0m                                                         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                                         \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                                                         loss, time_left,))\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "for epoch in range(0, EPOCH):\n",
    "    for i, (img, targets) in enumerate(data_loader):\n",
    "        # Model inputs\n",
    "        if CUDA:\n",
    "            img = Variable(img.cuda())\n",
    "            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "        \n",
    "        img = F.upsample(img, scale_factor=1/1.171875, mode='bilinear')\n",
    "        real_src_img = Variable(img.type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_src_img.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_src_img.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_trg_img = generator(real_src_img)\n",
    "        ssd_input_img = F.upsample(fake_trg_img, scale_factor=1.171875, mode='bilinear')\n",
    "        \n",
    "        #ssd_input_img = torch.cat((ssd_input_img, ssd_input_img, ssd_input_img), dim=1)\n",
    "        ssd_out = net(ssd_input_img)\n",
    "        optimizer_SSD.zero_grad()\n",
    "        #targets = torch.FloatTensor(bbx).cuda()\n",
    "        #targets = targets.unsqueeze(0)\n",
    "        #targets = targets.unsqueeze(0)\n",
    "        loss_l, loss_c = criterion(ssd_out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer_SSD.step()\n",
    "        \n",
    "        pred_fake = discriminator(fake_trg_img, real_src_img)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_trg_img, real_src_img)\n",
    "        #print(loss_pixel, loss_GAN, loss_bbx)\n",
    "\n",
    "        # Total loss\n",
    "        #loss_G = loss_GAN*0.5 + loss_pixel * lambda_pixel * 0.5 + loss\n",
    "        loss_G = loss\n",
    "\n",
    "        loss_G.backward(retain_graph=True)\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        '''optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = discriminator(fake_trg_img.detach(), real_src_img)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(fake_trg_img.detach(), real_src_img)\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()'''\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(data_loader) + i\n",
    "        batches_left = EPOCH * len(data_loader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        '''sys.stdout.write(\"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s [loss: %f]\" %\n",
    "                                                        (epoch, EPOCH,\n",
    "                                                        i, len(data_loader),\n",
    "                                                        loss_D.item(), loss_G.item(),\n",
    "                                                        loss_pixel.item(), loss_GAN.item(),\n",
    "                                                        time_left, \n",
    "                                                        loss))'''\n",
    "        sys.stdout.write(\"\\r[Epoch %d/%d] [Batch %d/%d] [G loss: %f] [loss: %f] ETA: %s \" %\n",
    "                                                        (epoch, EPOCH,\n",
    "                                                        i, len(data_loader),\n",
    "                                                        loss_G.item(),\n",
    "                                                        loss, time_left,))\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            #save_image(fake_trg_img.data[:1], 'images/%d.png' % batches_done, nrow=5, normalize=True)\n",
    "            sample_images(batches_done)\n",
    "\n",
    "\n",
    "    if SAVE_MODEL_EPOCH != -1 and epoch % SAVE_MODEL_EPOCH == 0:\n",
    "        # Save model checkpoints\n",
    "        print('Saving state, Epoch:', epoch)\n",
    "        torch.save(ssd_net.state_dict(),'saved_models/%s/ssd_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(generator.state_dict(), 'saved_models/%s/generator_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(discriminator.state_dict(), 'saved_models/%s/discriminator_%d.pth' % (dataset_name, epoch))\n",
    "torch.save(generator.state_dict(), 'saved_models/%s/generator.pth' % (dataset_name))\n",
    "torch.save(discriminator.state_dict(), 'saved_models/%s/discriminator.pth' % (dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
